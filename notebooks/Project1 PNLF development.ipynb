{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planetary Nebula <a class=\"tocSkip\">\n",
    "    \n",
    "This notebook is used to test and showcase the results of my first project. I use spectroscopic data from the [Multi Unit Spectroscopic Explorer](https://www.eso.org/sci/facilities/develop/instruments/muse.html) (MUSE) that has been observed as part of the [PHANGS](https://sites.google.com/view/phangs/home) collaboration.\n",
    "    \n",
    "I will use a set of line maps of emission lines to identify Planetary Nebula in the data an measure their brightness. This can then be used to fit an empiric relation and hence measure the distance to the galaxy.\n",
    "    \n",
    "This notebook is used for developement. Final code is moved to the `pnlf` packge in the `src` folder. Any production scripts reside in the `scripts` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    " \n",
    "### Load Basic Packages\n",
    "    \n",
    "First we load a bunch of common packages that are used across the project. More specific packages that are only used in one section are loaded later to make it clear where they belong to (this also applies to all custom moduls that were written for this project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pnlf.packages import *\n",
    "\n",
    "from pnlf.constants import tab10, single_column, two_column\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the `logging` module to handle informations and warnings (this does not always work as expected in jupyter notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout,format='%(levelname)s: %(message)s',level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "this uses the `ReadLineMaps` class from the `pnlf.io` module. To use it, we first need to specify the path to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.io import ReadLineMaps\n",
    "\n",
    "#with open(basedir / 'data' / 'interim' / 'parameters.json') as json_file:\n",
    "#    parameters = json.load(json_file)\n",
    "with open(basedir / 'data' / 'interim' / 'parameters.yml') as yml_file:\n",
    "    parameters = yaml.load(yml_file,Loader=yaml.FullLoader)\n",
    "    \n",
    "# table to save all results\n",
    "results = ascii.read(basedir/'data'/'interim'/ 'results.txt',format='fixed_width_two_line',delimiter_pad=' ',position_char='=')\n",
    "results.add_index('name')    \n",
    "\n",
    "name = 'NGC1385'\n",
    "\n",
    "# first we need to specify the path to the raw data\n",
    "basedir = Path('..')\n",
    "data_raw = Path('a:')\n",
    "#data_raw = Path('d:\\downloads\\MUSEDAP')\n",
    "#data_ext = Path('g:\\Archive')\n",
    "\n",
    "extensions = ['OIII5006', 'HA6562', 'NII6583', 'SII6716']\n",
    "\n",
    "# read in the data we will be working with and print some information\n",
    "galaxy = ReadLineMaps(data_raw/'MUSE_DR2.1'/'MUSEDAP',name,extensions,**parameters[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import circular_mask\n",
    "from pnlf.plot import create_RGB\n",
    "    \n",
    "mask = np.zeros(galaxy.shape,dtype=bool)\n",
    "mask |= galaxy.star_mask.astype(bool)\n",
    "\n",
    "if hasattr(galaxy,'mask'):\n",
    "    mask[circular_mask(*galaxy.shape,galaxy.center,radius=galaxy.mask)] = True\n",
    "\n",
    "img = create_RGB(galaxy.HA6562,galaxy.OIII5006_DAP,galaxy.SII6716,weights=[0.6,1,0.6],percentile=[95,99.,95])\n",
    "img[mask,...] = (1,1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = fig.add_subplot()\n",
    "\n",
    "#norm = simple_norm(galaxy.OIII5006_DAP,clip=False,max_percent=95)\n",
    "ax.imshow(img,origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Detection\n",
    "\n",
    "There are two different approaches to identifying sources in an image. The first utilizes PSF fitting and uses implementations from astropy. The other uses the external `SExtractor` package which detects peaks and classifies them with a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on IRAFStarFinder or DAOStarFinder\n",
    "\n",
    "The sources we are searching for are unresolved. However due to seeing, they will be smeared out. This PSF has the form of a Gaussian (or Moffat). The subsequent algorithms use this and try to fit a theoretical curve to the observed peaks in the image. If the fit aggrees within some threshold, it reports the peak as a source. The advantage is that for crowded fields, the algorithm will try to fit an individual function to each peak and thus enable us correctly identfiy objects that are closeby.\n",
    "\n",
    "The following function is based on this tutorial \n",
    "\n",
    "https://photutils.readthedocs.io/en/stable/detection.html\n",
    "\n",
    "https://photutils.readthedocs.io/en/stable/api/photutils.detection.DAOStarFinder.html#photutils.detection.DAOStarFinder\n",
    "\n",
    "**Requires**\n",
    " * A `photutils` starfinder. This can be either `DAOStarFinder` or `IRAFStarFinder`\n",
    " * `detect_unresolved_sources`\n",
    " \n",
    "**Returns**\n",
    " * `sources` a table with the position of all identified sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import DAOStarFinder            # DAOFIND routine to detect sources\n",
    "from photutils import IRAFStarFinder           # IRAF starfind routine to detect star\n",
    "\n",
    "from pnlf.detection import detect_unresolved_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we include all sources in this step and reject bad ones later\n",
    "sharplo   = 0.0 #galaxy.sharplo\n",
    "sharphi   = 1.0 #galaxy.sharphi\n",
    "roundness = 1.0 #galaxy.roundness\n",
    "\n",
    "sources = detect_unresolved_sources(galaxy,\n",
    "                                    'OIII5006_DAP',\n",
    "                                    StarFinder=DAOStarFinder,\n",
    "                                    threshold=galaxy.threshold,\n",
    "                                    exclude_region=mask,\n",
    "                                    oversize=1,\n",
    "                                    roundlo=-roundness,\n",
    "                                    roundhi=roundness,\n",
    "                                    sharplo=sharplo,\n",
    "                                    sharphi=sharphi,\n",
    "                                    save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Compare to Kreckel et al. 2017\n",
    "\n",
    "As mentioned in the beginning, we compare the newly detected sources to those from Kreckel et al. (2017). \n",
    "\n",
    "**Requires**\n",
    " * `match_coordinates_sky` from `astropy.coordinates` to compare the two catalogues.\n",
    " * `Angle` from `astropy.coordinates` to set a maximum seperation in units of arcseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky # match sources against existing catalog\n",
    "from astropy.coordinates import Angle                 # work with angles (e.g. 1°2′3″)\n",
    "\n",
    "tolerance = '0.5s'\n",
    "ID, angle, Quantity  = match_coordinates_sky(pn_NGC628_kreckel['SkyCoord'],sources['SkyCoord'])\n",
    "within_tolerance = len(angle[angle.__lt__(Angle(tolerance))])\n",
    "\n",
    "print(f'{within_tolerance} of {len(angle)} match within {tolerance}\": {within_tolerance / len(angle)*100:.1f} %')\n",
    "print(f'mean seperation is {angle[angle.__lt__(Angle(tolerance))].mean().to_string(u.arcsec,decimal=True)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Compare to Herrmann 2008 for NGC628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matchcoord = search_table(pn_herrmann,'M74')\n",
    "matchcoord['x'],matchcoord['y']= matchcoord['SkyCoord'].to_pixel(wcs=galaxy.wcs)\n",
    "\n",
    "matchcoord['in_frame'] = False\n",
    "x_dim,y_dim = galaxy.shape\n",
    "\n",
    "for row in matchcoord:\n",
    "    txt,x,y = row['ID'], row['x'], row['y']    \n",
    "    if 0<=int(x)<x_dim and 0<=int(y)<y_dim:\n",
    "        if not np.isnan(galaxy.PSF[int(x),int(y)]):\n",
    "            row['in_frame'] = True\n",
    "\n",
    "matchcoord = matchcoord[matchcoord['in_frame']]\n",
    "#matchcoord = matchcoord[matchcoord['in_frame']]\n",
    "#data = [d for d in matchcoord['m5007']]\n",
    "#data = matchcoord[matchcoord['in_frame']]['m5007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.974,6.974))\n",
    "ax1 = fig.add_subplot(111,projection=galaxy.wcs)\n",
    "\n",
    "norm = simple_norm(galaxy.OIII5006_DAP,'linear',clip=False,max_percent=95)\n",
    "ax1.imshow(galaxy.OIII5006_DAP,norm=norm,cmap=plt.cm.Blues_r)\n",
    "\n",
    "ax1.scatter(matchcoord['x'],matchcoord['y'],marker='o',s=2,c='tab:red')\n",
    "for row in matchcoord:\n",
    "    txt,x,y = row['ID'], row['x']+5, row['y']    \n",
    "    ax1.annotate(txt, (x, y),fontsize=4,color='tab:red')\n",
    "    \n",
    "plt.savefig(basedir / 'reports' / 'NGC628_Herrmann.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky # match sources against existing catalog\n",
    "from astropy.coordinates import Angle                 # work with angles (e.g. 1°2′3″)\n",
    "\n",
    "# ((tbl['type']=='PN') | (tbl['type']=='SNR')) & \n",
    "catalogcoord = tbl[(tbl['mOIII']<27.5)]\n",
    "    \n",
    "tolerance = '1s'\n",
    "ID, angle, Quantity  = match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n",
    "within_tolerance = len(angle[angle.__lt__(Angle(tolerance))])\n",
    "\n",
    "print(f'{within_tolerance} of {len(angle)} match within {tolerance}\": {within_tolerance / len(angle)*100:.1f} %')\n",
    "print(f'mean seperation is {angle[angle.__lt__(Angle(tolerance))].mean().to_string(u.arcsec,decimal=True)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx, angle, quan = match_coordinates_sky(SkyCoord('01h36m41.76s','15d47m57.4s'),catalogcoord['SkyCoord'])\n",
    "catalogcoord[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from photutils import CircularAperture\n",
    "from pymuse.plot.plot import create_RGB\n",
    "rgb = create_RGB(galaxy.SII6716,galaxy.HA6562,galaxy.OIII5006_DAP,percentile=96)\n",
    "\n",
    "fig = plt.figure(figsize=(6.974,6.974))\n",
    "ax1 = fig.add_subplot(111,projection=galaxy.wcs)\n",
    "\n",
    "norm = simple_norm(galaxy.OIII5006_DAP,'linear',clip=False,max_percent=95)\n",
    "#ax1.imshow(galaxy.OIII5006_DAP,norm=norm,cmap=plt.cm.Blues)\n",
    "ax1.imshow(rgb)\n",
    "\n",
    "positions = np.transpose([matchcoord['x'],matchcoord['y']])\n",
    "apertures = CircularAperture(positions, r=6)\n",
    "apertures.plot(color='tab:red',lw=.3, alpha=1,ax=ax1)\n",
    "\n",
    "positions = np.transpose([catalogcoord['x'],catalogcoord['y']])\n",
    "apertures = CircularAperture(positions, r=6)\n",
    "apertures.plot(color='tab:orange',lw=.3, alpha=1,ax=ax1)\n",
    "\n",
    "for row in matchcoord:\n",
    "    txt,x,y = row['ID'], row['x']+5, row['y']    \n",
    "    ax1.annotate(txt, (x, y),fontsize=4,color='tab:red')\n",
    "    \n",
    "plt.savefig(basedir / 'reports' / 'NGC628_Herrmann.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matchcoord['type'] = catalogcoord[ID]['type']\n",
    "matchcoord['sep']  = angle\n",
    "matchcoord['mOIII'] = catalogcoord[ID]['mOIII']\n",
    "matchcoord['dmOIII'] = catalogcoord[ID]['dmOIII']\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# we only use sources when their position agrees within this tolerance\n",
    "tolerance = '1.0\"'\n",
    "\n",
    "ax.errorbar(matchcoord[matchcoord['sep'].__lt__(Angle(tolerance))]['m5007'],\n",
    "            matchcoord[matchcoord['sep'].__lt__(Angle(tolerance))]['mOIII'],\n",
    "            yerr=3*matchcoord[matchcoord['sep'].__lt__(Angle(tolerance))]['dmOIII'],\n",
    "            xerr=0.15,\n",
    "            fmt='o')\n",
    "\n",
    "ax.scatter(matchcoord[matchcoord['sep'].__gt__(Angle(tolerance))]['m5007'],\n",
    "           matchcoord[matchcoord['sep'].__gt__(Angle(tolerance))]['m5007'],color='tab:red')\n",
    "\n",
    "ax.plot([25.3,27.5],[25.3,27.5],color='black',lw=0.4)\n",
    "ax.plot([25.3,27.5],[24.8,27.],color='black',lw=0.2,ls='--')\n",
    "ax.plot([25.3,27.5],[25.8,28.],color='black',lw=0.2,ls='--')\n",
    "ax.set_xlabel(r'$\\mathrm{m}_{[\\mathrm{OIII}]}$ Herrmann et al. 2008',fontsize=16)\n",
    "ax.set_ylabel(r'$\\mathrm{m}_{[\\mathrm{OIII}]}$ this work',fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Compare to Herrmann 2008 for NGC5068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matchcoord = pn_NGC5068\n",
    "matchcoord['x'],matchcoord['y']= matchcoord['SkyCoord'].to_pixel(wcs=galaxy.wcs)\n",
    "\n",
    "matchcoord['in_frame'] = False\n",
    "x_dim,y_dim = galaxy.shape\n",
    "\n",
    "for row in matchcoord:\n",
    "    txt,x,y = row['ID'], row['x'], row['y']    \n",
    "    if 0<=int(x)<x_dim and 0<=int(y)<y_dim:\n",
    "        if not np.isnan(galaxy.PSF[int(x),int(y)]):\n",
    "            row['in_frame'] = True\n",
    "\n",
    "matchcoord = matchcoord[matchcoord['in_frame']]\n",
    "#matchcoord = matchcoord[matchcoord['in_frame']]\n",
    "#data = [d for d in matchcoord['m5007']]\n",
    "#data = matchcoord[matchcoord['in_frame']]['m5007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky # match sources against existing catalog\n",
    "from astropy.coordinates import Angle                 # work with angles (e.g. 1°2′3″)\n",
    "\n",
    "# ((tbl['type']=='PN') | (tbl['type']=='SNR')) & \n",
    "catalogcoord = tbl[(tbl['mOIII']<28.5)]\n",
    "    \n",
    "tolerance = '1s'\n",
    "ID, angle, Quantity  = match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n",
    "within_tolerance = len(angle[angle.__lt__(Angle(tolerance))])\n",
    "\n",
    "print(f'{within_tolerance} of {len(angle)} match within {tolerance}\": {within_tolerance / len(angle)*100:.1f} %')\n",
    "print(f'mean seperation is {angle[angle.__lt__(Angle(tolerance))].mean().to_string(u.arcsec,decimal=True)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.974,6.974))\n",
    "ax1 = fig.add_subplot(111,projection=galaxy.wcs)\n",
    "\n",
    "norm = simple_norm(galaxy.OIII5006_DAP,'linear',clip=False,max_percent=95)\n",
    "ax1.imshow(galaxy.OIII5006_DAP,norm=norm,cmap=plt.cm.Blues_r)\n",
    "\n",
    "ax1.scatter(matchcoord['x'],matchcoord['y'],marker='o',s=2,c='tab:red')\n",
    "for row in matchcoord:\n",
    "    txt,x,y = row['ID'], row['x']+5, row['y']    \n",
    "    ax1.annotate(txt, (x, y),fontsize=4,color='tab:red')\n",
    "    \n",
    "#plt.savefig(basedir / 'reports' / 'NGC5068_Herrmann.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from photutils import CircularAperture\n",
    "from pymuse.plot.plot import create_RGB\n",
    "rgb = create_RGB(galaxy.SII6716,galaxy.HA6562,galaxy.OIII5006_DAP,percentile=96)\n",
    "\n",
    "fig = plt.figure(figsize=(6.974,6.974))\n",
    "ax1 = fig.add_subplot(111,projection=galaxy.wcs)\n",
    "\n",
    "norm = simple_norm(galaxy.OIII5006_DAP,'linear',clip=False,max_percent=95)\n",
    "#ax1.imshow(galaxy.OIII5006_DAP,norm=norm,cmap=plt.cm.Blues)\n",
    "ax1.imshow(rgb)\n",
    "\n",
    "positions = np.transpose([matchcoord['x'],matchcoord['y']])\n",
    "apertures = CircularAperture(positions, r=6)\n",
    "apertures.plot(color='tab:red',lw=.3, alpha=1,ax=ax1)\n",
    "\n",
    "positions = np.transpose([catalogcoord['x'],catalogcoord['y']])\n",
    "apertures = CircularAperture(positions, r=6)\n",
    "apertures.plot(color='tab:orange',lw=.3, alpha=1,ax=ax1)\n",
    "\n",
    "for row in matchcoord:\n",
    "    txt,x,y = row['ID'], row['x']+5, row['y']    \n",
    "    ax1.annotate(txt, (x, y),fontsize=4,color='tab:red')\n",
    "    \n",
    "plt.savefig(basedir / 'reports' / 'NGC5068_Herrmann.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Plot detected sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pymuse.plot.plot import plot_sky_with_detected_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "position = np.transpose((sources['x'], sources['y']))\n",
    "positions_kk = np.transpose(pn_bright['SkyCoord'].to_pixel(wcs=galaxy.wcs))\n",
    "positions = (position,positions_kk)\n",
    "\n",
    "save_file = Path.cwd() / '..' / 'reports' / 'figures' / f'{galaxy.name}_sky_sources_DAO.pdf'\n",
    "plot_sky_with_detected_stars(data=galaxy.OIII5006_DAP,\n",
    "                             wcs=galaxy.wcs,\n",
    "                             positions=positions,\n",
    "                             filename=save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Cut out detected stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pymuse.plot.plot import sample_cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_file = Path.cwd() / '..' / 'reports' / 'figures' / f'{galaxy.name}_stars.pdf'\n",
    "\n",
    "stars = sample_cutouts(galaxy.OIII5006_DAP,sources,galaxy.wcs,nrows=4,ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using SExtractor\n",
    "\n",
    "there is no Python implementation of SExtractor. Instead we run it from the command line\n",
    "\n",
    "```\n",
    "sextractor file.fits -c default.sex\n",
    "```\n",
    "\n",
    "this will produce a file `test.cat` which contains the position of the sources. We read this table and calculate the sky position wiht astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_fwhm(x,y):\n",
    "    try:\n",
    "        return galaxy.PSF[int(y),int(x)]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "file = data_raw /  'NGC628.cat'\n",
    "\n",
    "sources = ascii.read(file)\n",
    "sources.rename_columns(['X_IMAGE','Y_IMAGE'],['x','y'])\n",
    "sources['fwhm'] = np.array([get_fwhm(x,y) for x,y in zip(sources['x'],sources['y'])])\n",
    "sources['sharpness']  = 0.3\n",
    "sources['roundness2'] = 0.3\n",
    "\n",
    "print(f'{len(sources)} sources found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Match with known sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ID, angle, Quantity  = match_coordinates_sky(pn_bright['SkyCoord'],table['SkyCoord'])\n",
    "within_1_arcsec = len(angle[angle.__lt__(Angle(\"0.5s\"))])\n",
    "\n",
    "print(f'{within_1_arcsec} of {len(angle)} match within 0.5\": {within_1_arcsec / len(angle)*100:.1f} %')\n",
    "print(f'mean seperation is {angle.mean().to_string(u.arcsec,decimal=True)}\"')\n",
    "#print(f'mean angle: {angle.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plot detected sources\n",
    "\n",
    "this requires the previously loaded `plot_sources` from `pymuse.plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file = Path.cwd() / '..' / 'reports' / 'figures' / f'{NGC628.name}_sources_sextractor.pdf'\n",
    "\n",
    "position = np.transpose((sources['x'], sources['y']))\n",
    "references = np.transpose(pn_bright['SkyCoord'].to_pixel(wcs=NGC628.wcs))\n",
    "positions = (position,references)\n",
    "\n",
    "sky_with_detected_stars(data=NGC628.OIII5006_old,wcs=NGC628.wcs,positions=positions,filename=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Completeness limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pymuse.detection import completeness_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mock_sources = completeness_limit(\n",
    "                    galaxy,\n",
    "                    'OIII5006',\n",
    "                    DAOStarFinder,\n",
    "                    iterations=10,\n",
    "                    threshold=threshold,\n",
    "                    exclude_region=mask,\n",
    "                    oversize=oversize,\n",
    "                    roundlo=-roundness,\n",
    "                    roundhi=roundness,\n",
    "                    sharplo=sharplo,\n",
    "                    sharphi=sharphi,\n",
    "                    exclude_border=True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux measurement\n",
    "\n",
    "In the previous step we detected potential PN candidates by their [OIII] emission. This means we know their position but lack exact flux measurments. In this section we measure the flux of the identified objects in different emission lines that are used in later steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth curve analysis\n",
    "\n",
    "\n",
    "\n",
    "#### Gaussian\n",
    "\n",
    "A shape that is commonly used for the PSF is that of a 2D gaussian (we assume  variance of $\\sigma_x^2 = \\sigma_y^2 = \\sigma^2$). If we center the peak at the origin the PSF is described by\n",
    "$$\n",
    "f(x,y) = A \\exp\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "with some amplitude $A$. We can rewrite this in polar coordinates as \n",
    "$$\n",
    "f(r,\\phi) = A \\exp\\left(-\\frac{r^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "The light inside an aperture of radius $P(R)$ is given by the integral\n",
    "$$\n",
    "P(R) = \\int_0^{2\\pi} \\int_0^R f(r,\\phi) \\mathrm{d} \\phi r \\mathrm{d} r = 2\\pi \\sigma^2 A \\left(1-\\exp \\left(-\\frac{R^2}{2\\sigma^2}\\right) \\right) \n",
    "$$\n",
    "We are interested in the ratio $p(R) = P(R) / P(\\infty)$. If we use the relation between the standard deviation and the $\\mathrm{FWHM}$ of a Gaussian $\\sigma = \\frac{\\mathrm{FWHM}}{2\\sqrt{2\\ln2}}$, we can write\n",
    "$$\n",
    "\\begin{align}p(R) = 1-\\exp\\left(- \\frac{4 \\ln 2 \\cdot R^2}{\\mathrm{fwhm}^2} \\right)\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Moffat\n",
    "\n",
    "The measured FWHM are systematically larger than the reported values. A possible cause is that the shape of the PSF is not a perfect Gaussian, but rather described by a [Moffat](https://en.wikipedia.org/wiki/Moffat_distribution). This distribution is larger towards the wings and fitting a Gaussian to such a shape should result in a larger FWHM\n",
    "\n",
    "$$\n",
    "f(R;\\alpha,\\gamma) = A \\left[1 + \\left(\\frac{R}{\\gamma}\\right)^2 \\right]^{- \\alpha}\n",
    "$$\n",
    "\n",
    "**Note**: this nomenclature follows `astropy` and contradicts the commonly used scheme which uses $\\gamma=\\alpha$ and $\\alpha=\\beta$.\n",
    "\n",
    "The Full Width Half Maximum of this function is given by\n",
    "$$\n",
    "\\mathrm{FWHM} = 2\\gamma \\sqrt{2^{1/\\alpha}-1}\n",
    "$$\n",
    "\n",
    "like we did for the Gaussian we can calculate the amount of flux within a radius R as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(R) = \\int_0^{2\\pi} \\int_0^R f(r,\\phi) \\mathrm{d} \\phi r \\mathrm{d} r = 2\\pi \\int_0^R A \\left[1 + \\left(\\frac{r}{\\gamma}\\right)^2 \\right]^{- \\alpha} r \\mathrm{d} r\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "to solve this we substitute $u=1+\\left(\\frac{r}{\\gamma} \\right)^2 $ with $\\frac{\\mathrm{d} u}{\\mathrm{d} r} = \\frac{2r}{\\gamma^2}$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(R) = 2\\pi A \\frac{\\gamma^2}{2(1-\\alpha)} \\left[1 + \\left( \\frac{R}{\\gamma} \\right)^2 \\right]^{1-\\alpha}- 2\\pi A\\frac{\\gamma^2}{2(1-\\alpha)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "again we are interested in the ratio $p(R) = P(R) / P(\\infty)$. If we assume that $\\alpha>1$, the first term will be $0$ for $R\\rightarrow \\infty$ and so we end up with \n",
    "\n",
    "$$\n",
    "p(r) = \\left[ 1+\\left( \\frac{R}{\\gamma}\\right)^2\\right]^{1-\\alpha} - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import light_in_gaussian, light_in_moffat, fwhm_moffat\n",
    "\n",
    "alpha = 4\n",
    "gamma = 10\n",
    "fwhm = fwhm_moffat(alpha,gamma)\n",
    "print(f'alpha={alpha:.2f}, gamma={gamma:.2f}, fwhm={fwhm:.2f}')\n",
    "\n",
    "d = np.arange(0,20,0.2)\n",
    "g = light_in_gaussian(d,fwhm)\n",
    "m = light_in_moffat(d,alpha,gamma)\n",
    "plt.plot(d/fwhm,100*g,label='Gaussian')\n",
    "plt.plot(d/fwhm,100*m,label='Moffat')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('diameter in fwhm')\n",
    "plt.ylabel('light in aperture in %')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an ideal Gaussian/Moffat source and measure growth curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling import models, fitting \n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.stats import gaussian_sigma_to_fwhm, gaussian_fwhm_to_sigma\n",
    "\n",
    "from pnlf.photometry import growth_curve\n",
    "from pnlf.auxiliary import fwhm_moffat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=64\n",
    "fwhm = 10\n",
    "bkg = 0.5\n",
    "print(f'fwhm={fwhm}')\n",
    "\n",
    "std =  fwhm * gaussian_fwhm_to_sigma\n",
    "gaussian = models.Gaussian2D(x_mean=size/2,y_mean=size/2,x_stddev=std,y_stddev=std)\n",
    "img = gaussian(*np.indices((size,size))) + np.random.uniform(0,bkg,(size,size))\n",
    "plt.imshow(img, origin='lower')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwhm_fit = growth_curve(img,size/2,size/2,model='gaussian',plot=True)[0]\n",
    "print(f'fwhm={fwhm}, measured={fwhm_fit:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=64\n",
    "alpha = 2.8\n",
    "gamma = 4\n",
    "#gamma = 6/(2*np.sqrt(2**(1/4.76)-1))\n",
    "bkg = 0.01\n",
    "\n",
    "fwhm = 2*gamma * np.sqrt(2**(1/alpha)-1)\n",
    "print(f'alpha={alpha:.2f}, gamma={gamma:.2f}, fwhm={fwhm:.2f}')\n",
    "\n",
    "std = 4 * gaussian_fwhm_to_sigma\n",
    "moffat = models.Moffat2D(x_0=size/2,y_0=size/2,alpha=alpha,gamma=gamma)\n",
    "img = moffat(*np.indices((size,size))) + np.random.uniform(0,bkg,(size,size))\n",
    "#plt.imshow(img, origin='lower')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.photometry import measure_single_flux\n",
    "\n",
    "def test_growth_curve(alpha=2.8,gamma=4,bkg=0.01):\n",
    "    '''test the growth curve\n",
    "    \n",
    "    measure the flux with the correct FWHM and values that are too large/small\n",
    "    '''\n",
    "    \n",
    "    # create a mock image with a perfect moffat and some background\n",
    "    size = 64\n",
    "    moffat = models.Moffat2D(x_0=size/2,y_0=size/2,alpha=alpha,gamma=gamma)\n",
    "    img = moffat(*np.indices((size,size))) + np.random.uniform(0,bkg,(size,size))\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1,ncols=3,figsize=(7,3))\n",
    "    axes_iter = iter(axes.flatten())\n",
    "    \n",
    "    for dfwhm in [-0.5,0.0,0.5]:\n",
    "        fwhm = 2 * gamma * np.sqrt(2**(1/alpha)-1) + dfwhm\n",
    "        flux = []\n",
    "        radii = np.arange(1,3.75,0.5)\n",
    "        for aperture_size in radii:\n",
    "            flux.append(measure_single_flux(img,[size/2,size/2],aperture_size,alpha,fwhm/(2*np.sqrt(2**(1/alpha)-1))))\n",
    "\n",
    "        flux=np.array(flux) \n",
    "        \n",
    "        print(f'{np.max(np.abs((flux[radii>=1.5]-flux[-1])/flux[-1]*100)):.2f}')\n",
    "            \n",
    "        ax = next(axes_iter)\n",
    "        ax.axvline(1.5,color='black',lw=0.8)\n",
    "        ax.axhline(-1,color='black',lw=0.8)\n",
    "        ax.axhline(1,color='black',lw=0.8)\n",
    "        \n",
    "        ax.scatter(radii,(flux-flux[-1])/flux[-1]*100,s=5)\n",
    "\n",
    "        ax.set_title(f'FWHM={fwhm:.2f}, Ftot={flux[-1]:.2f}')\n",
    "        ax.set(xlabel='aperture radius / fwhm',ylim=[-15,15])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "test_growth_curve(alpha=2.8,gamma=4,bkg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = growth_curve(img,size/2,size/2,model='moffat',plot=True,length=10)\n",
    "fwhm_fit = fit[0]\n",
    "fwhm_fit = fwhm_moffat(*fit)\n",
    "\n",
    "#print(f'fwhm={fwhm:.2f}, fit={fwhm_fit:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare PSF and aperture photometry\n",
    "\n",
    "http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/aperture_phot2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling import models \n",
    "from astropy.stats import gaussian_sigma_to_fwhm, gaussian_fwhm_to_sigma\n",
    "from pnlf.photometry import measure_single_flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/astropy/photutils/issues/558\n",
    "from photutils.psf import IterativelySubtractedPSFPhotometry, BasicPSFPhotometry, DAOPhotPSFPhotometry\n",
    "from photutils.psf import IntegratedGaussianPRF, DAOGroup\n",
    "from photutils.background import MMMBackground, MADStdBackgroundRMS\n",
    "from astropy.modeling.fitting import LevMarLSQFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=30\n",
    "alpha = 2.8\n",
    "gamma = 4\n",
    "amplitude=5\n",
    "bkg = 0.00\n",
    "\n",
    "fwhm = 2*gamma * np.sqrt(2**(1/alpha)-1)\n",
    "moffat = models.Moffat2D(amplitude=amplitude,x_0=size/2,y_0=size/2,alpha=alpha,gamma=gamma)\n",
    "img = moffat(*np.indices((size,size))) + np.random.uniform(0,bkg,(size,size))\n",
    "\n",
    "flux_0 = measure_single_flux(img,[size/2,size/2],aperture_size=2,model='Moffat',alpha=alpha,gamma=gamma)\n",
    "\n",
    "pos = Table(names=['x_0', 'y_0'], data=[[size/2],[size/2]])\n",
    "pos['id'] = np.arange(1,len(pos)+1)\n",
    "pos['flux_0'] = [flux_0]\n",
    "\n",
    "print(f'alpha={alpha:.2f}, gamma={gamma:.2f}, fwhm={fwhm:.2f}, flux0={flux_0:.2f}')\n",
    "# the total flux is given by 2*np.pi*amplitude*gamma**2/(2*(alpha-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.psf import prepare_psf_model\n",
    "\n",
    "psffunc = models.Moffat2D(amplitude=1, gamma=gamma, alpha=alpha, x_0=0, y_0=0)\n",
    "psffunc.amplitude.fixed=False\n",
    "psffunc.gamma.fixed=True\n",
    "psffunc.alpha.fixed=True\n",
    "psfmodel = prepare_psf_model(psffunc, xname='x_0', yname='y_0', fluxname=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry = BasicPSFPhotometry(group_maker     = DAOGroup(fwhm*3),\n",
    "                                bkg_estimator   = MMMBackground(),\n",
    "                                psf_model       = psfmodel,\n",
    "                                fitter          = LevMarLSQFitter(),\n",
    "                                fitshape        = (15,15))\n",
    "result_tab = photometry(image=img, init_guesses=pos)\n",
    "\n",
    "print(f\"aperture={flux_0:.2f}, PSF={result_tab['flux_fit'][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicPSFPhotometry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 2*np.pi\n",
    "f = \n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry = DAOPhotPSFPhotometry(crit_separation = fwhm*3,\n",
    "                                  threshold       = 0.5,\n",
    "                                  fwhm            = fwhm,\n",
    "                                  psf_model       = psfmodel,\n",
    "                                  fitter          = LevMarLSQFitter(),\n",
    "                                  fitshape        = (11,11),\n",
    "                                  aperture_radius = 3*fwhm)\n",
    "result_tab = photometry(image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for a Gaussian (maybe the fit works here)\n",
    "\n",
    "from photutils.psf import IntegratedGaussianPRF\n",
    "\n",
    "amplitude=10\n",
    "size=64\n",
    "fwhm = 4\n",
    "bkg = 0.1\n",
    "\n",
    "std =  fwhm * gaussian_fwhm_to_sigma\n",
    "gaussian = models.Gaussian2D(amplitude=amplitude,x_mean=size/2,y_mean=size/2,x_stddev=std,y_stddev=std)\n",
    "img = gaussian(*np.indices((size,size))) + np.random.uniform(0,bkg,(size,size))\n",
    "\n",
    "flux_0 = measure_single_flux(img,[size/2,size/2],aperture_size=2,model='Gaussian',fwhm=fwhm)\n",
    "\n",
    "pos = Table(names=['x_0', 'y_0'], data=[[size/2],[size/2]])\n",
    "pos['id'] = np.arange(1,len(pos)+1)\n",
    "#pos['flux_0'] = [flux_0]\n",
    "fwhm+=0.5\n",
    "\n",
    "photometry = BasicPSFPhotometry(group_maker     = DAOGroup(fwhm*2),\n",
    "                                bkg_estimator   = MMMBackground(),\n",
    "                                psf_model       = IntegratedGaussianPRF(sigma=fwhm*gaussian_fwhm_to_sigma),\n",
    "                                fitter          = LevMarLSQFitter(),\n",
    "                                finder          = DAOStarFinder(threshold=0.5*np.max(img),fwhm=fwhm),\n",
    "                                fitshape        = (7,7))\n",
    "\n",
    "\n",
    "result_tab = photometry(image=img)\n",
    "flux_fit = result_tab['flux_fit'][0]\n",
    "print(f\"aperture={flux_0:.2f}, PSF={flux_fit:.2f}, dif={100*(flux_0-flux_fit)/flux_0:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply to real data\n",
    "\n",
    "We saw that we can predict the aperture size dependence of the flux for mock sources. Now we pick real objects and try to do the same. \n",
    "\n",
    "Stars should have much larger stellar velocities. We use this to identify potential foreground stars in our source catalogue. We cannot use the peaks of the velocity maps as they seem to be displaced from the center of the star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.stats import gaussian_fwhm_to_sigma\n",
    "\n",
    "from pymuse.plot import single_cutout\n",
    "from pymuse.photometry import growth_curve, correct_PSF, fwhm_moffat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we find stars due to their high velocity dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel for smoothing\n",
    "smoothing_length = 10 \n",
    "kernel = np.ones((smoothing_length,smoothing_length))\n",
    "\n",
    "# find stars by their large velocity dispersion\n",
    "star_mask = np.zeros(NGC628.V_STARS.shape,dtype='f8')\n",
    "star_mask[np.abs(NGC628.V_STARS)>200] = 1\n",
    "\n",
    "# smooth ouput with convolution\n",
    "star_mask = convolve(star_mask,kernel,mode='same')\n",
    "star_mask[star_mask>0.1] = 1\n",
    "star_mask[star_mask<0.1] = 0\n",
    "star_mask[np.isnan(NGC628.V_STARS)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = detect_unresolved_sources(NGC628,\n",
    "                                  'whitelight',\n",
    "                                   StarFinder=DAOStarFinder,\n",
    "                                   threshold=5,\n",
    "                                   oversize_PSF = 1.,\n",
    "                                   save=False)\n",
    "    \n",
    "stars = stars[star_mask[stars['y'].astype(int),stars['x'].astype(int)]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "size = 20\n",
    "x,y,fwhm = stars[i][['x','y','fwhm']]\n",
    "single_cutout(NGC628,'whitelight',x,y,size=size)\n",
    "print(fwhm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NGC628.HA6562\n",
    "\n",
    "aperture = 25\n",
    "fit = growth_curve(data,x,y,model='moffat',rmax=15,plot=True,length=10)\n",
    "#fwhm_fit=fit[0]\n",
    "fwhm_fit = fwhm_moffat(*fit)\n",
    "\n",
    "radius = np.arange(0,10,0.5)\n",
    "plt.plot(radius,light_in_gaussian(radius,fwhm),label='gaussian',ls='--',color='tab:red')\n",
    "plt.legend()\n",
    "\n",
    "print(f'reported={fwhm:.2f}, measured={fwhm_fit:.2f}, ratio={fwhm_fit/fwhm:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 6 objects classified as stars in our field of view. For each of them we do a growth curve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20\n",
    "data = NGC628.whitelight\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "for i in range(len(stars)):\n",
    "    x,y,fwhm = stars[i][['x','y','fwhm']]\n",
    "\n",
    "    fit = growth_curve(data,x,y,model='moffat',rmax=30,plot=True,length=10)\n",
    "    fwhm_fit = fwhm_moffat(*fit)\n",
    "    print(f'alpha={fit[0]:.2f}, gamma={fit[1]:.2f}, reported={fwhm:.2f}, measured={fwhm_fit:.2f}, ratio={fwhm_fit/fwhm:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit 2D Function\n",
    "\n",
    "So far we didn't fit the PSF shape directly but took a slight detour with the light inside an aperture. Here we try to fit the 2D functions to the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 16\n",
    "\n",
    "#sub = sources[sources['peak']>500]\n",
    "#x,y,fwhm = sub[8][['x','y','fwhm']]\n",
    "\n",
    "for line, cmap in zip(['whitelight','OIII5006','HA6562'],[plt.cm.viridis,plt.cm.Blues_r,plt.cm.Reds_r]):\n",
    "    # defien the size of the cutout region\n",
    "    star = Cutout2D(getattr(NGC628,line), (x,y), u.Quantity((size, size), u.pixel))\n",
    "\n",
    "    fitter = fitting.LevMarLSQFitter()\n",
    "    data = star.data\n",
    "    fig ,(ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize=(12,3))\n",
    "    #cmap = plt.cm.Blues\n",
    "\n",
    "    ax1.imshow(data,origin='lower',cmap=cmap)\n",
    "    ax1.set_title(f'image {line}')\n",
    "\n",
    "    std = fwhm * gaussian_fwhm_to_sigma\n",
    "    gaussian_theory = models.Gaussian2D(x_mean=size/2,y_mean=size/2,x_stddev=std,y_stddev=std)\n",
    "    ax2.imshow(gaussian_theory(*np.indices(data.shape)), origin='lower',cmap=cmap)\n",
    "    ax2.set_title('Gaussian reported')\n",
    "    \n",
    "    model = models.Gaussian2D()\n",
    "    gaussian = fitter(model,*np.indices(data.shape),data,maxiter=1000)\n",
    "    ax3.imshow(gaussian(*np.indices(data.shape)), origin='lower',cmap=cmap)\n",
    "    ax3.set_title('Gaussian fit')\n",
    "\n",
    "    model = models.Moffat2D(alpha=4.765,fixed={'alpha':True}) \n",
    "    moffat = fitter(model,*np.indices(data.shape),data,maxiter=2000)\n",
    "    fwhm_moffat = 2*moffat.gamma.value * np.sqrt(2**(1/moffat.alpha.value)-1)\n",
    "    ax4.imshow(moffat(*np.indices(data.shape)), origin='lower',cmap=cmap)\n",
    "    ax4.set_title('Moffat fit')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    xstd = gaussian.x_stddev * gaussian_sigma_to_fwhm\n",
    "    ystd = gaussian.y_stddev * gaussian_sigma_to_fwhm\n",
    "\n",
    "    print(f'{line}: reported: {fwhm:.3f}, gaussian={xstd:.3f} ,{ystd:.3f}, moffat={fwhm_moffat:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_photometry(data,x,y,r,r_in,r_out):\n",
    "    '''\n",
    "    performe aperture photometry with backround subtraction for one source\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        image data\n",
    "        \n",
    "    x : float\n",
    "        x cooridinate of the source\n",
    "    \n",
    "    y : float\n",
    "        y cooridinate of the source\n",
    "        \n",
    "    r : float\n",
    "        radius of the main aperture\n",
    "    \n",
    "    r_in : float\n",
    "        inner radius of the annulus that is used for the background        \n",
    "    \n",
    "    r_out : float\n",
    "        outer radius of the annulus that is used for the background\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    aperture = CircularAperture((x,y), r=r)\n",
    "    annulus_aperture = CircularAnnulus((x,y), r_in=r_in, r_out=r_out)\n",
    "    mask = annulus_aperture.to_mask(method='center')\n",
    "    annulus_data = mask.multiply(data)\n",
    "    annulus_data_1d = annulus_data[mask.data > 0]\n",
    "    _, bkg_median, _ = sigma_clipped_stats(annulus_data_1d[~np.isnan(annulus_data_1d)])\n",
    "    phot = aperture_photometry(data,aperture)\n",
    "    \n",
    "    return phot['aperture_sum'][0]-aperture.area*bkg_median\n",
    "\n",
    "def aperture_correction(data,positions,r_aperture):\n",
    "    \n",
    "    fluxes = [fast_photometry(data,x,y,r,r_in,r_out) for x,y in positions]\n",
    "    \n",
    "    apertures = CircularAperture(positions,r_aperture)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from pymuse.plot import single_cutout\n",
    "\n",
    "positions = np.transpose([sources['x'],sources['y']])\n",
    "\n",
    "tree = cKDTree(positions)\n",
    "dists = tree.query(positions, 2)\n",
    "nn_dist = dists[0][:, 1]\n",
    "\n",
    "sub = sources[(nn_dist>16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from photutils import Background2D\n",
    "\n",
    "mask = np.isnan(galaxy.HA6562)\n",
    "bkg = Background2D(galaxy.HA6562,(10,10), \n",
    "                filter_size=(5,5),\n",
    "                #sigma_clip= SigmaClip(sigma=3.,maxiters=None), \n",
    "                #bkg_estimator=SExtractorBackground(),\n",
    "                mask=mask).background\n",
    "bkg[mask] = np.nan\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "\n",
    "norm = simple_norm(galaxy.HA6562,clip=False,percent=95)\n",
    "ax1.imshow(galaxy.HA6562,norm=norm,origin='lower',cmap=plt.cm.Reds)\n",
    "\n",
    "#norm = simple_norm(bkg,clip=False,max_percent=95)\n",
    "ax2.imshow(bkg,norm=norm,origin='lower',cmap=plt.cm.Reds)\n",
    "\n",
    "#norm = simple_norm(galaxy.HA6562-bkg,clip=False,max_percent=95)\n",
    "ax3.imshow(galaxy.HA6562-bkg,norm=norm,origin='lower',cmap=plt.cm.Reds)\n",
    "\n",
    "plt.savefig(basedir/'reports'/'background.pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aperture Photometry\n",
    "\n",
    "we use the positions of the previously detected sources to measure the flux of different lines\n",
    "\n",
    "https://photutils.readthedocs.io/en/stable/aperture.html\n",
    "\n",
    "the values in the pixels are in units of $10^{-20} \\ \\mathrm{erg}  \\ \\mathrm{cm}^{-2} \\ \\mathrm{s}^{-1} / \\mathrm{spaxel}$. For the [OIII] line, this flux is then converted to an apparent magnitude\n",
    "$$\n",
    "m_{[\\mathrm{O\\ III}]} = -2.5 \\cdot \\log F_{[\\mathrm{O\\ III}]} - 13.74\n",
    "$$\n",
    "\n",
    "where $F_{[\\mathrm{O\\ III}]}$ is given in $\\mathrm{erg}  \\ \\mathrm{cm}^{-2} \\ \\mathrm{s}^{-1}$. Error propagation gives the error of the magnitude as\n",
    "\n",
    "$$\n",
    "\\Delta m_{[\\mathrm{O\\ III}]} = \\sqrt{\\left(\\frac{-2.5 \\cdot \\Delta F_{[\\mathrm{O\\ III}]}}{\\ln 10 \\cdot F_{[\\mathrm{O\\ III}]}}\\right)^2 }\n",
    "$$\n",
    "\n",
    "We only correct for extinction in the milky way. therefor we use the extinction function from Cardelli, Clayton & Mathis (1989) with $A_V = 0.2$ and $R_V=3.1$. The extinction is calculated with the following package\n",
    "\n",
    "https://extinction.readthedocs.io/en/latest/\n",
    "\n",
    "(Note: the DAP products are already extinction corrected).\n",
    "\n",
    "**Requires**\n",
    " * `extinction` a python package to account for the extinction in the Milky Way.\n",
    " * `measure_flux` from `pnlf.photometry`\n",
    " \n",
    "**Returns**\n",
    " * `flux` a Table with the measured line fluxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky # match sources against existing catalog\n",
    "from astropy.coordinates import Angle                 # work with angles (e.g. 1°2′3″)\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "from extinction import ccm89     # calculate extinction Cardelli et al. (1989)\n",
    "from dust_extinction.parameter_averages import CCM89\n",
    "\n",
    "from pnlf.photometry import measure_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extinction correction with Astropy\n",
    "#extinction_model = CCM89(Rv=Rv)\n",
    "#extinction = -2.5*np.log10(extinction_model.extinguish(500.7*u.nanometer,Ebv=Ebv))\n",
    "#print(f'Av = {extinction:.2f}')\n",
    "#flux['mOIII'] = flux['mOIII'] - extinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aperture_size = 2#galaxy.aperturesize\n",
    "\n",
    "flux = measure_flux(galaxy,\n",
    "                    sources,\n",
    "                    alpha=galaxy.power_index,\n",
    "                    Rv=3.1,\n",
    "                    Ebv=galaxy.Ebv,\n",
    "                    extinction='MW',\n",
    "                    background='local',\n",
    "                    aperture_size=aperture_size)\n",
    "\n",
    "# calculate astronomical coordinates for comparison\n",
    "\n",
    "# calculate magnitudes from measured fluxes\n",
    "flux['mOIII'] = -2.5*np.log10(flux['OIII5006']*1e-20) - 13.74\n",
    "flux['dmOIII'] = np.abs( 2.5/np.log(10) * flux['OIII5006_err'] / flux['OIII5006'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different backgrounds\n",
    "loc  = -2.5*np.log10(1e-20 * (flux['HA6562_aperture_sum'] - flux['HA6562_bkg_local']))  - 13.74\n",
    "glob = -2.5*np.log10(1e-20 * (flux['HA6562_aperture_sum'] - flux['HA6562_bkg_global']))  - 13.74\n",
    "con = -2.5*np.log10(1e-20 * (flux['HA6562_aperture_sum'] - flux['HA6562_bkg_convole']))  - 13.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "plt.scatter(loc,glob,marker='o',s=2,color=tab10[0])\n",
    "ax.set_xlabel(r'$m_{\\mathrm{H}\\alpha}$ (local bkg)')\n",
    "ax.set_ylabel(r'$m_{\\mathrm{H}\\alpha}$ (global bkg)')\n",
    "\n",
    "plt.plot([20,30],[19.5,29.5],'gray',ls='--',lw=0.5)\n",
    "plt.plot([20,30],[20.5,30.5],'gray',ls='--',lw=0.5)\n",
    "\n",
    "plt.plot([20,30],[20,30],'black',ls='-',lw=0.5)\n",
    "plt.xlim([20,30])\n",
    "plt.ylim([20,30])\n",
    "\n",
    "#plt.savefig('../../notes/img/global_bkg.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare DAP vs Kreckel maps\n",
    "plt.scatter(-2.5*np.log10(flux['OIII5006']*1e-20) - 13.74,-2.5*np.log10(flux['OIII5006_DAP']*1e-20) - 13.74)\n",
    "plt.plot([24,29],[24,29],color='grey',ls='--')\n",
    "plt.xlim([24,29])\n",
    "plt.ylim([24,29])\n",
    "plt.title('mOIII for sources in NGC628')\n",
    "plt.xlabel('sum')\n",
    "plt.ylabel('fit (DAP)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different FWHM\n",
    "\n",
    "we assume an uncertainty of dFWHM = 0.1\"=0.5px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources['fwhm'] += 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aperture_size = 2#galaxy.aperturesize\n",
    "\n",
    "flux2 = measure_flux(galaxy,sources,alpha=galaxy.power_index,Rv=3.1,Ebv=galaxy.Ebv,\n",
    "                    extinction='MW',background='local',aperture_size=aperture_size)\n",
    "\n",
    "\n",
    "flux2['mOIII'] = -2.5*np.log10(flux2['OIII5006']*1e-20) - 13.74\n",
    "flux2['dmOIII'] = np.abs( 2.5/np.log(10) * flux2['OIII5006_err'] / flux2['OIII5006'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(flux['mOIII'],flux2['mOIII'])\n",
    "xmin,xmax = ax1.get_xlim()\n",
    "ymin,ymax = ax1.get_ylim()\n",
    "lim = max(xmax,ymax)\n",
    "ax1.plot([20,lim],[20,lim])\n",
    "ax1.set(xlim=(20,lim),ylim=(20,lim))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare aperture sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rv  = 3.1\n",
    "Ebv = 0.062\n",
    "aperture_size2 = 1.5\n",
    "\n",
    "flux2 = measure_flux(galaxy,\n",
    "                    sources,\n",
    "                    alpha=galaxy.power_index,\n",
    "                    Rv=Rv,\n",
    "                    Ebv=Ebv,\n",
    "                    extinction='MW',\n",
    "                    background='local',\n",
    "                    aperture_size=aperture_size2)\n",
    "\n",
    "# calculate astronomical coordinates for comparison\n",
    "\n",
    "# calculate magnitudes from measured fluxes\n",
    "flux2['mOIII'] = -2.5*np.log10(flux2['OIII5006']*1e-20) - 13.74\n",
    "flux2['dmOIII'] = np.abs( 2.5/np.log(10) * flux2['OIII5006_err'] / flux2['OIII5006'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,figsize=(4,4))\n",
    "ax.scatter(flux[tbl['type']=='PN']['mOIII'],flux2[tbl['type']=='PN']['mOIII'],s=2)\n",
    "xmin,xmax,ymin,ymax=25.,31,25.,31\n",
    "comp=28\n",
    "ax.plot([xmin,xmax],[xmin,xmax],color='black',lw=0.4)\n",
    "ax.plot([xmin,xmax],[xmin-0.2,xmax-0.2],color='gray',lw=0.5,ls='--')\n",
    "ax.plot([xmin,xmax],[xmin+0.2,xmax+0.2],color='gray',lw=0.5,ls='--')\n",
    "ax.plot([xmin,comp,comp],[comp,comp,ymin],color='black',lw=0.4)\n",
    "ax.set(xlabel=f'mOIII aps={aperture_size} FWHM',ylabel=f'mOIII aps={aperture_size2} FWHM',xlim=[xmin,xmax],ylim=[ymin,ymax])\n",
    "plt.savefig(basedir/'reports'/galaxy.name/'aperture_size.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Compare to Kreckel et al. 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky # match sources against existing catalog\n",
    "from astropy.coordinates import Angle                 # work with angles (e.g. 1°2′3″)\n",
    "from astropy.table import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ID, angle, Quantity  = match_coordinates_sky(pn_NGC628_kreckel['SkyCoord'],tbl['SkyCoord'])\n",
    "#tbl[ID][angle.__lt__(Angle('0.5\"'))].show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pn_bright = pn_NGC628_kreckel[pn_NGC628_kreckel['mOIII']<28]\n",
    "\n",
    "ID, angle, Quantity  = match_coordinates_sky(pn_bright['SkyCoord'],flux['SkyCoord'])\n",
    "\n",
    "# for each object from Kreckel et al. 2017, we search for the nearest source\n",
    "# and copy our measured quantities to compare the two\n",
    "pn_bright['mOIII_measured']  = flux[ID]['mOIII']\n",
    "pn_bright['dmOIII_measured'] = flux[ID]['dmOIII']\n",
    "pn_bright['sep'] = angle\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# we only use sources when their position agrees within this tolerance\n",
    "tolerance = '0.5\"'\n",
    "\n",
    "# calculate the difference in magnitude for those objects\n",
    "dif = np.mean(np.abs(pn_bright[angle<Angle(tolerance)]['mOIII'] - pn_bright[angle<Angle(tolerance)]['mOIII_measured']))\n",
    "\n",
    "print(f'{len(pn_bright[angle<Angle(tolerance)])} PN match within {tolerance}')\n",
    "print(f'the mean deviation is {dif:.3f} dex')\n",
    "\n",
    "ax.errorbar(pn_bright[angle<Angle(tolerance)]['mOIII'],\n",
    "            pn_bright[angle<Angle(tolerance)]['mOIII_measured'],\n",
    "            yerr=pn_bright[angle<Angle(tolerance)]['dmOIII_measured'],\n",
    "            fmt='o')\n",
    "\n",
    "ax.scatter(pn_bright[angle>Angle(tolerance)]['mOIII'],pn_bright[angle>Angle(tolerance)]['mOIII'],color='tab:orange')\n",
    "\n",
    "ax.plot([25.5,27.5],[25.5,27.5],color='black',lw=0.4)\n",
    "ax.plot([25.5,27.5],[25.,27.],color='black',lw=0.2,ls='--')\n",
    "ax.plot([25.5,27.5],[26.,28.],color='black',lw=0.2,ls='--')\n",
    "ax.set_xlabel(r'$\\mathrm{m}_{[\\mathrm{OIII}]}$ Kreckel et al. 2017',fontsize=16)\n",
    "ax.set_ylabel(r'$\\mathrm{m}_{[\\mathrm{OIII}]}$ this work',fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pymuse.plot.plot import single_cutout\n",
    "x,y=pn_kreckel[np.argmax(pn_kreckel['mOIII'])]['SkyCoord'].to_pixel(wcs=galaxy.wcs)\n",
    "\n",
    "single_cutout(galaxy,'OIII5006_DAP',x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "while the OIII magnitudes agree fairly well, we see a huge discrepancy in the H$\\alpha$ fluxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table = hstack([pn_bright,flux[ID]])\n",
    "\n",
    "for col in ['OIII5006','HA6562','SII6716','NII6583']:\n",
    "    table[col][table[col]<0] = table[f'{col}_err'][table[col]<0] \n",
    "    \n",
    "table['OIII/Ha_measured'] = table['OIII5006'] / table['HA6562']\n",
    "table['Ha/SII_measured'] = table['HA6562'] / table['SII6716']\n",
    "table['Ha/NII_measured'] = table['HA6562'] / table['NII6583']\n",
    "\n",
    "    \n",
    "# print all relevant columns\n",
    "table[table['sep']<Angle('0.5s')][['mOIII_1','mOIII_2','OIII/Ha','OIII/Ha_measured','Ha/SII','Ha/SII_measured','Ha/NII','Ha/NII_measured']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(tbl['HA6562_apbkg'],tbl['HA6562']/tbl['HA6562_err'])\n",
    "plt.xlabel(r'$\\mathrm{H}\\alpha$ Background',fontsize=16)\n",
    "plt.ylabel(r'$\\mathrm{H}\\alpha / \\mathrm{H}\\alpha_\\mathrm{err}$',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(tbl['HA6562_apbkg'],tbl['HA6562']/tbl['HA6562_apbkg'])\n",
    "plt.xlabel(r'$\\mathrm{H}\\alpha$ Background',fontsize=16)\n",
    "plt.ylabel(r'$\\mathrm{H}\\alpha / \\mathrm{H}\\alpha_\\mathrm{err}$',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.sum(tbl['HA6562']/tbl['HA6562_err']<3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dust extinction etc.\n",
    "\n",
    "The V-band extinction $A_V$ and the color excess $E(B-V)=(B-V)_\\text{obs} - (B-V)_\\text{int}$ are related via the selective extinction\n",
    "$$\n",
    "A_V = R_V E(B-V)\n",
    "$$\n",
    "The extinction at wavelength $\\lambda$ can be obtained with the extinction curve\n",
    "$$\n",
    "k(\\lambda) = \\frac{A_\\lambda}{E(B-V)}\\quad \\rightarrow\\quad A_\\lambda = k(\\lambda) E(B-V)\n",
    "$$\n",
    "or \n",
    "$$\n",
    "k(\\lambda) = \\frac{A_\\lambda}{A_V} R_V\n",
    "$$\n",
    "The color excess can be calculated if the intrinsic ratio of two lines is known\n",
    "$$\n",
    "\\begin{align}\n",
    "E(B-V) &= \\frac{E(F_2-F_1)}{k(\\lambda_2) -k(\\lambda_1)} \\\\\n",
    "&=\\frac{2.5}{k(\\lambda_2) - k(\\lambda_1)} \\log_{10} \\left[ \\frac{(F_1 / F_2)_\\text{obs}}{(F_1 / F_2)_\\text{int}} \\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "`dust_extinction.evaluate` returns $A_\\lambda / A_V$, hence we need to multiply with $R_V$ to get $k(\\lambda)$\n",
    "\n",
    "\n",
    "`dust_extinction.extinguish` returns the fractional extinction $f_\\lambda$. To get $A_\\lambda$ we need to $-2.5\\log_{10} f_\\lambda$\n",
    "\n",
    "$$\n",
    "E(B-V)_\\text{star} = 0.44 E(B-V)_\\text{nebula}\n",
    "$$\n",
    "\n",
    "#### Attenuation vs extinction\n",
    "\n",
    "extinction only considers one ray of light. Therefor it will only decrease the intensity. For attenuation, a larger area and a bundle of light rays is considered. It is possible that light is scattered from one line of sight into another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import CCM89, F99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Rv=3.1\n",
    "\n",
    "ext_model = CCM89(Rv=Rv)\n",
    "k = lambda lam: ext_model.evaluate(lam*u.angstrom,Rv) * Rv\n",
    "\n",
    "def calculateEbv(F1,F2,lam1,lam2,Rint):\n",
    "    '''\n",
    "    \n",
    "    Rint : float\n",
    "        intrinsic flux ratio flux1/flux2\n",
    "    '''\n",
    "        \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        Ebv = 2.5 / (k(lam2)-k(lam1)) * np.log10(F1/F2/Rint)\n",
    "    \n",
    "    Ebv[~np.isfinite(Ebv)] = np.nan\n",
    "    \n",
    "    return Ebv\n",
    "\n",
    "Ebv = calculateEbv(galaxy.HA6562,galaxy.HB4861,6562,4861,2.86)\n",
    "Ebv[(galaxy.HB4861 / galaxy.HB4861_err < 5) & (galaxy.HA6562 / galaxy.HA6562_err < 15)] = np.nan\n",
    "\n",
    "Av = Ebv * Rv\n",
    "\n",
    "\n",
    "norm = simple_norm(Av,'linear',clip=False,min_cut=-5,max_cut=5)\n",
    "im = plt.imshow(Av,origin='lower',norm=norm)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "row = sources[1]\n",
    "\n",
    "Av = np.array([galaxy.Av[int(row['x']),int(row['y'])] for row in sources])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ext_model.extinguish(5007*u.angstrom,Av=Av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x*ext_model.evaluate(5007*u.angstrom,Rv=3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x=0.\n",
    "10**(-0.4*x*k(5007)/Rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ext_model.extinguish(5007*u.angstrom,Av=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "external = Path('g:/Archive')\n",
    "\n",
    "with fits.open(external / 'MUSE' / 'AUXILIARY' / 'AVmaps' / 'fits' / f'{galaxy.name}_AV_caseB_negnan_conv_broad_Ha15_Hb5.fits') as hdul:\n",
    "    AVmap = hdul[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#norm = simple_norm(AVmap,'linear',clip=False,percent=99.9)\n",
    "im = plt.imshow(AVmap,origin='lower',norm=norm)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dif = np.abs((AVmap-Av)/Av)\n",
    "norm = simple_norm(dif,'linear',clip=False,min_cut=0,max_cut=1)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "im = plt.imshow(dif,origin='lower',norm=norm)\n",
    "plt.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "box_size = 4\n",
    "xmax, ymax = Av.shape\n",
    "\n",
    "out = np.zeros(Av.shape)\n",
    "\n",
    "for i in range(xmax):\n",
    "    for j in range(ymax):\n",
    "        mask = slice(max(0,i-box_size),min(i+box_size,xmax)),slice(max(0,j-box_size),min(j+box_size,ymax))        \n",
    "        out[i,j] = np.nanmean(Av[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.nanmean(out[(out<np.inf) & (out>-np.inf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lam = 5007\n",
    "Alam =  ext_model.evaluate(lam*u.angstrom,Rv) * AVmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission line diagnostics\n",
    "\n",
    "We built a catalgoue of possible planetary nebula and measuerd different emission lines. However this catalogue still contains objects that are similar to PN like HII regions or supernova remenants (SNR). In this next step we use emission line diagnostics to eliminate those contanimations. The distance modulus $\\mu$ is defined as the difference between the apparent and the absolute magnitude. By definition of the absolute magnitude, this relates to the distance $d$ in parsec as \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu = m - M \\\\\n",
    "d = 10^{1+\\frac{\\mu}{5}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " 1. filter out HII regions\n",
    "    $$\n",
    "     4 > \\log_{10} \\frac{[\\mathrm{OIII}]}{\\mathrm{H}\\alpha +[\\mathrm{NII}]} > -0.37 M_{[\\mathrm{OIII}]} - 1.16\n",
    "    $$\n",
    " 2. filter out SNR\n",
    "    $$\n",
    "     \\mathrm{H}\\alpha / [\\mathrm{SII}] < 2.5\n",
    "    $$\n",
    "    \n",
    " 3. estimate completness limit and remove fainter sources\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.analyse import emission_line_diagnostics\n",
    "\n",
    "print(f'emission line diagnostics for {galaxy.name}')\n",
    "print(f'mu={galaxy.mu:.2f}, cl={galaxy.completeness_limit}')\n",
    "tbl = emission_line_diagnostics(flux,galaxy.mu,galaxy.completeness_limit) \n",
    "\n",
    "# create additional columns that are needed for the classification\n",
    "tbl['sharp'] = sources['sharpness']\n",
    "tbl['round'] = sources['roundness2']\n",
    "tbl['SkyCoord'] = SkyCoord.from_pixel(tbl['x'],tbl['y'],galaxy.wcs)\n",
    "\n",
    "tbl['exclude'] = False\n",
    "\n",
    "cut=0\n",
    "slow    = galaxy.sharplo  \n",
    "shigh   = galaxy.sharphi \n",
    "r       = galaxy.roundness \n",
    "if cut>0:\n",
    "    logger.warning('you are using a cut')\n",
    "    \n",
    "#slope = []\n",
    "#for row in tbl:\n",
    "#    star = Cutout2D(galaxy.OIII5006, (row['x'],row['y']), u.Quantity((size, size), u.pixel),wcs=galaxy.wcs)\n",
    "#    profile = radial_profile(star.data,star.input_position_cutout)\n",
    "#    slope.append(np.sum(np.ediff1d(profile)>0) / len(profile))\n",
    "#tbl['slope'] = slope    \n",
    "\n",
    "# table contains all detected objects. here we mask all undesired objects.\n",
    "c_shape = ((tbl['sharp']>slow) & (tbl['sharp']<shigh) & (np.abs(tbl['round'])<r)) #& (tbl['OIII5006']>10*np.abs(tbl['OIII5006_bkg_local']))\n",
    "c_PN    = (tbl['type']=='PN')\n",
    "c_SNR   = (tbl['SNRorPN'] & (tbl['type']=='SNR'))\n",
    "c_AV    = ((tbl['Av']<0.4) | np.isnan(tbl['Av']))\n",
    "c_cut   = (cut<tbl['mOIII'])\n",
    "c_detec = tbl['OIII5006_detection'] \n",
    "c_limit = (tbl['mOIII']<galaxy.completeness_limit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = basedir / 'data' / 'catalogues' / f'pn_candidates_{galaxy.name}.txt'\n",
    "with open(filename,'w',newline='\\n') as f:\n",
    "    tbl['RaDec'] = tbl['SkyCoord'].to_string(style='hmsdms',precision=2)\n",
    "    for col in tbl.colnames:\n",
    "        if col not in ['id','RaDec','type']:\n",
    "            tbl[col].info.format = '%.3f' \n",
    "    ascii.write(tbl[['id','type','x','y','RaDec','OIII5006','OIII5006_err','mOIII','dmOIII',\n",
    "                     'HA6562','HA6562_err','HA6562_detection','NII6583','NII6583_err','NII6583_detection',\n",
    "                     'SII6716','SII6716_err','SII6716_detection']][tbl['type']!='NaN'],\n",
    "                f,format='fixed_width',delimiter='\\t',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlike H$\\alpha$, [OIII] and H$\\beta$ have similar wavelenghts and hence are equally affected by extinction. It might therefore be better to use this line in the first criteria (we can use the theoretical ratio H$\\alpha$ = 2.8 H$\\beta$ to substitute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in ['PN','HII','SNR']:\n",
    "    tmp = tbl[tbl['type']==t]\n",
    "    ax.scatter(tmp['MOIII'],tmp['OIII5006']/tmp['HB4861'],label=t)\n",
    "    \n",
    "MOIII = np.linspace(-5,-1)\n",
    "ax.plot(MOIII,10**(-0.37*MOIII-0.7),color='black')\n",
    "ax.legend()\n",
    "    \n",
    "ax.set(xlim=[-5,-1],ylim=[1e-1,200],yscale='log')\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:.16g}'.format(y)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "tmp = tbl[(10**(-0.37*tbl['MOIII']-0.7)<tbl['OIII5006']/tbl['HB4861']) & (tbl['type']=='HII')]\n",
    "ax.scatter(tmp['MOIII'],tmp['OIII5006']/tmp['HB4861'],label='HII',color='tab:blue')\n",
    "    \n",
    "tmp = tbl[(10**(-0.37*tbl['MOIII']-0.7)>tbl['OIII5006']/tbl['HB4861']) & (tbl['type']=='PN')]\n",
    "ax.scatter(tmp['MOIII'],tmp['OIII5006']/tmp['HB4861'],label='PN',color='tab:red')\n",
    "    \n",
    "MOIII = np.linspace(-4.5,0)\n",
    "ax.plot(MOIII,10**(-0.37*MOIII-0.7),color='black')\n",
    "ax.legend()\n",
    "    \n",
    "ax.set(xlim=[-4.5,0],ylim=[1e-1,200],yscale='log')\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:.16g}'.format(y)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot.plot import cutout_with_profile\n",
    "\n",
    "tmp = tbl[(10**(-0.37*tbl['MOIII']-0.7)>tbl['OIII5006']/tbl['HB4861']) & (tbl['type']=='PN')]\n",
    "cutout_with_profile(galaxy,table=tmp,size=40,diagnostics=False,filename=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tbl[(10**(-0.37*tbl['MOIII']-0.7)<tbl['OIII5006']/tbl['HB4861']) & (tbl['type']=='HII')]\n",
    "tmp.sort('mOIII')\n",
    "cutout_with_profile(galaxy,table=tmp,size=40,diagnostics=False,filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare velocity dispersion of PN to HII-regions and SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "completeness = 27.5\n",
    "m = np.nanmean(tbl[tbl['mOIII']<completeness]['OIII5006_SIGMA'])\n",
    "print(f'all: v_sig = {m}')\n",
    "\n",
    "for t in ['PN','HII','SNR']:\n",
    "    m = np.nanmean(tbl[(tbl['type']==t) & (tbl['mOIII']<completeness)]['OIII5006_SIGMA'])\n",
    "    std = np.nanstd(tbl[(tbl['type']==t) & (tbl['mOIII']<completeness)]['OIII5006_SIGMA'])\n",
    "\n",
    "    print(f'{t}: v_sig = {m:.2f} +- {std:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the result of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pnlf.plot.pnlf import plot_emission_line_ratio\n",
    "plot_emission_line_ratio(tbl,mu = 29.91,completeness=27.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot sky with source type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from photutils import CircularAperture\n",
    "from astropy.visualization import simple_norm\n",
    "\n",
    "# ====== define input parameters =============================\n",
    "galaxy = NGC628\n",
    "labels=['SII6716','HA6562','OIII5006']\n",
    "wcs=NGC628.wcs\n",
    "# ============================================================\n",
    "\n",
    "table = tbl\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(20,10),subplot_kw={'projection':wcs})\n",
    "\n",
    "norm = simple_norm(galaxy.HA6562,'linear',clip=False,max_percent=95)\n",
    "ax1.imshow(galaxy.HA6562,norm=norm,cmap=plt.cm.Greens_r)\n",
    "\n",
    "norm = simple_norm(galaxy.OIII5006_DAP,'linear',clip=False,max_percent=95)\n",
    "ax2.imshow(galaxy.OIII5006_DAP,norm=norm,cmap=plt.cm.Blues_r)\n",
    "\n",
    "for t,c in zip(['HII','SNR','PN'],['black','red','yellow']):\n",
    "    \n",
    "    sub = table[table['type']==t]\n",
    "    positions = np.transpose([sub['x'],sub['y']])\n",
    "    apertures = CircularAperture(positions, r=6)\n",
    "    apertures.plot(color=c,lw=.5, alpha=1,ax=ax1)\n",
    "    apertures.plot(color=c,lw=.5, alpha=1,ax=ax2)\n",
    "\n",
    "ax1.set_title('HA6562')\n",
    "ax2.set_title('OIII5006')\n",
    "\n",
    "plt.savefig(basedir / 'reports' / 'figures' / 'NGC628_detections_classification.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare classification to the results from Francesco Santoro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymuse.detection import match_catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir / 'data' / 'external' / 'FS_cat_v01.fits') as hdul:\n",
    "    cat_FS = Table(hdul[1].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for PNe that were classified by Francescos in my catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tbl['SkyCoord']\n",
    "PNe_candidates = cat_FS[(cat_FS['gal_name']=='NGC628') & (cat_FS['PNe_candidate']==1)]\n",
    "idx, sep = match_catalogues(PNe_candidates[['cen_x','cen_y']],tbl[['x','y']])\n",
    "\n",
    "max_sep = 2\n",
    "print(f'sep < {max_sep} px: {sum(sep<max_sep)/len(sep)*100:.2f} %')\n",
    "tbl[(idx)][sep<max_sep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the measured fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = hstack([PNe_candidates,tbl[(idx)][sep<max_sep]])\n",
    "\n",
    "tmp[['OIII5006_FLUX', 'OIII5006', 'HA6562_FLUX', 'HA6562', 'NII6583_FLUX','NII6583', 'SII6716_FLUX' ,'SII6716']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for HII regions that were classified by me in Francescos catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HII_candidates = tbl[tbl['type'] == 'HII']\n",
    "catalogue = cat_FS[(~np.isnan(cat_FS['cen_x'])) & (~np.isnan(cat_FS['cen_y'])) & (cat_FS['gal_name']=='NGC628')]\n",
    "idx, sep = match_catalogues(HII_candidates[['x','y']],catalogue[['cen_x','cen_y']])\n",
    "\n",
    "max_sep = 2\n",
    "print(f'sep < 1 px: {sum(sep<1)/len(sep)*100:.2f} %')\n",
    "catalogue[idx][sep<max_sep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Enrico\n",
    "\n",
    "this is a list of objects that enrico classified differently. Here we check where they fall in our diagnostics and if they could impact the measured distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com[(com['Class_Fabian']=='PN') & (com['Class_Enrico']=='hii')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, join\n",
    "from pnlf.plot.pnlf import plot_emission_line_ratio\n",
    "\n",
    "com=ascii.read(basedir/'data'/'external'/'Combined_classification.csv')\n",
    "cat=ascii.read(basedir/'data'/'external'/'deprecated'/'NGC0628_nebulae.txt')\n",
    "com.rename_column('ID_fabian','id')\n",
    "\n",
    "sub = com[(com['Class_Fabian']=='PN') & (com['Class_Enrico']=='hii')]\n",
    "\n",
    "joined = join(sub,cat,keys='id')\n",
    "joined.rename_columns(['x_1','y_1'],['x','y'])\n",
    "joined['HA6562_detection']=True\n",
    "joined['SNRorPN'] = False\n",
    "joined['overluminous']=False\n",
    "joined['v_SIGMA_S/N'] = 20\n",
    "joined['v_SIGMA'] = 20\n",
    "joined['OIII5006'] = 10**(-(joined['mOIII']+13.74)/2.5)/1e-20\n",
    "joined['SII_detection'] = True\n",
    "\n",
    "plot_emission_line_ratio(joined,29.96,completeness=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planetary nebula luminosity function\n",
    "\n",
    "The absolute magnitude of PN is described by (this is an empirical relation)\n",
    "$$ \n",
    "\\begin{align}\n",
    "N(M) &\\propto e^{0.307 M} \\left( 1- e^{3(M^*-M)} \\right) \\\\\n",
    "&\\propto e^{0.307 (m-\\mu)} \\left( 1- e^{3(M^*-m+\\mu)} \\right) \\\\\n",
    "&\\propto e^{0.307 (m-\\mu)} - e^{3M^*-2.693(m-\\mu)} \n",
    "\\end{align}\n",
    "$$\n",
    "To use this function in our Maximum Likelihood we need to normalize it. The indefinite integral is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int N(m)\\; \\mathrm{d} m \\propto \\frac{e^{0.307(m-\\mu)}}{0.307} + \\frac{e^{3M^* - 2.693(m-\\mu)}}{2.693}\n",
    "\\end{align}\n",
    "$$\n",
    "The luminosity function has a root when $M^* - m + \\mu =0$. We use this for the lower bound normalization. For the upper bound we use the luminosity for which we are confindent to detect all sources (=completeness limit).\n",
    "\n",
    "We can already use the normalized luminosity function for the maximum likelihood fitting. However we cannot really illustrate the result. To do this we need to introduce some binning. Then we can show the fit similar to a curve fit. Because we sum the PN in the bins, we don't plot the luminosity function but the integrated function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "m = np.linspace(-5,-1,1000)\n",
    "p = pnlf(m,0,-1)\n",
    "\n",
    "a = -4.47+2.5\n",
    "b = 2\n",
    "s= 1/(1+np.exp(b*(m-a))**2)\n",
    "logistic = 1*(s-min(s))/(max(s)-min(s)) # normalize function to 0-1\n",
    "\n",
    "ax.plot(m,p,color='tab:red')\n",
    "ax.plot(m,p*s,color='tab:red',ls='--')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r'$M_{[\\mathrm{OIII}]}$ / mag')\n",
    "ax.set_ylabel(r'$N / N_{tot}$')\n",
    "ax.set_ylim([0.01,0.5])\n",
    "ax.set_xlim([-5,-1])\n",
    "\n",
    "ax.axvline(-4.47,lw=0.5,ls='--',color='black')\n",
    "\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:.2g}'.format(y)))\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.25))\n",
    "plt.savefig(basedir / 'reports' / 'pnlf.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With maximum likelihood\n",
    "\n",
    "**Note**: the function which is used for the likelihood must be normalized\n",
    "\n",
    "useful site: \n",
    "\n",
    "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1\n",
    "\n",
    "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm \n",
    "\n",
    "http://personal.psu.edu/abs12/stat504/Lecture/lec3_4up.pdf\n",
    "\n",
    "https://emcee.readthedocs.io/en/stable/tutorials/line/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(mu):\n",
    "    mu0 = 29.91\n",
    "    std = 0.1\n",
    "    \n",
    "    return 1 / (std*np.sqrt(2*np.pi)) * np.exp(-(mu-mu0)**2 / (2*std**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pnlf.analyse import MaximumLikelihood1D, PNLF, pnlf\n",
    "from pnlf.plot.pnlf import plot_pnlf\n",
    "\n",
    "completeness = galaxy.completeness_limit\n",
    "\n",
    "criteria = ((tbl['type']=='PN'))\n",
    "data = tbl[np.where(criteria & (tbl['mOIII']<completeness))]['mOIII']\n",
    "err  = tbl[np.where(criteria & (tbl['mOIII']<completeness))]['dmOIII']\n",
    "#data = data[data>27]\n",
    "\n",
    "fitter = MaximumLikelihood1D(pnlf,\n",
    "                           data,\n",
    "                           #prior=prior,\n",
    "                           #err=err,\n",
    "                           mhigh=completeness)\n",
    "\n",
    "# a good guess would be mu_guess = min(data)-Mmax\n",
    "galaxy.mu,dp,dm = fitter([28])\n",
    "fig = fitter.plot()\n",
    "\n",
    "plot_pnlf(data,galaxy.mu,completeness,binsize=galaxy.binsize,mhigh=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data from Kreckel+2017\n",
    "completeness= 27.5\n",
    "\n",
    "raw = vstack([pn_kreckel['mOIII'], snr_kreckel['mOIII']])['mOIII']\n",
    "data = raw[raw<completeness]\n",
    "\n",
    "fitter = MaximumLikelihood1D(pnlf,\n",
    "                             data,\n",
    "                             mhigh=completeness)\n",
    "\n",
    "# a good guess would be mu_guess = min(data)-Mmax\n",
    "mu,dp,dm = fitter([28])\n",
    "\n",
    "plot_pnlf(raw,mu,completeness,binsize=0.4,mhigh=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the fit\n",
    "\n",
    "to plot the fit we need to bin the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot.pnlf import plot_pnlf\n",
    "filename = basedir / 'reports' / f'{galaxy.name}_PNLF'\n",
    "\n",
    "plot_pnlf(tbl[criteria]['mOIII'],galaxy.mu,completeness,binsize=0.5,mhigh=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot.plot import plot_sky_with_detected_stars, sample_cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import CircularAperture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tbl[np.where(criteria & (tbl['mOIII']<28.5))]\n",
    "pos = np.transpose((tmp['x'],tmp['y']))\n",
    "\n",
    "save_file = Path.cwd() / '..' / 'reports' / f'{galaxy.name}_map_PNLF.pdf'\n",
    "plot_sky_with_detected_stars(data=galaxy.OIII5006_DAP,\n",
    "                             wcs=galaxy.wcs,\n",
    "                             positions=pos,\n",
    "                             filename=save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metallicity dependence of the zeropoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnlf_Mmax(m,Mmax,mu,mhigh):\n",
    "\n",
    "    m = np.atleast_1d(m)\n",
    "    mlow = Mmax+mu\n",
    "    \n",
    "    normalization = 1/(F(mhigh,mu) - F(mlow,mu))    \n",
    "    out = normalization * np.exp(0.307*(m-mu)) * (1-np.exp(3*(Mmax-m+mu)))\n",
    "    out[(m>mhigh) | (m<mlow)] = 0\n",
    "    \n",
    "    return out\n",
    "\n",
    "def gaussian(x,mu,sig):\n",
    "    return 1/np.sqrt(2*np.pi*sig**2) * np.exp(-(x-mu)**2/(2*sig**2))\n",
    "\n",
    "def prior(param):\n",
    "    return gaussian(param,-4.47,1)\n",
    "\n",
    "\n",
    "mu_trgb = 29.73\n",
    "fitter = MaximumLikelihood1D(pnlf_Mmax,data,err=err,prior=prior,mu=mu_trgb,mhigh=galaxy.completeness_limit)\n",
    "Mmax = minimize(fitter.likelihood,[-4.47],method=fitter.method).x[0]\n",
    "\n",
    "print(f'Mmax={Mmax:.2f}, dMmax={Mmax+4.47:.2f}')\n",
    "\n",
    "\n",
    "#Plot PNLF\n",
    "filename = None\n",
    "axes = plot_pnlf(tbl[criteria]['mOIII'],mu_trgb,galaxy.completeness_limit,\n",
    "                 binsize=binsize,mhigh=28.5,Mmax=Mmax,filename=filename,color=tab10[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import kstest\n",
    "from pnlf.analyse import sample_pnlf\n",
    "\n",
    "def pnlf_cum(m,mu,mhigh):\n",
    "    \n",
    "    Nbins = 1000\n",
    "    x =np.linspace(mu-4.47,mhigh,Nbins)\n",
    "    cdf = np.cumsum(pnlf(x,mu,mhigh))*(mhigh-mu+4.47)/Nbins\n",
    "        \n",
    "    return np.interp(m,cdf,x)\n",
    "\n",
    "\n",
    "sampled_data = sample_pnlf(1000,galaxy.mu,galaxy.completeness_limit)\n",
    "\n",
    "#print(kstest(data,pnlf_cum,args=(galaxy.mu,galaxy.completeness_limit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results:\n",
    "    n = row['name']\n",
    "    d = ascii.read(basedir/'data'/'catalogues'/f'{n}_PN_candidates.txt',format='fixed_width_two_line')\n",
    "    mu = row['(m-M)']\n",
    "    cl = parameters[n]['completeness_limit']\n",
    "    sampled_data = sample_pnlf(10000,mu,cl)\n",
    "    ks = ks_2samp(d['mOIII'],sampled_data)\n",
    "\n",
    "    print(f'{n}: D={ks.statistic:.2f}, pvalue={ks.pvalue:.2f}')\n",
    "\n",
    "#ks.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_modulus = 29.96\n",
    "completeness = 28\n",
    "\n",
    "    \n",
    "\n",
    "sampled_data = sample_pnlf(1000,distance_modulus,completeness)\n",
    "\n",
    "axes = plot_pnlf(sampled_data,\n",
    "                 distance_modulus,\n",
    "                 completeness,\n",
    "                 binsize=binsize,\n",
    "                 color=tab10[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With external data\n",
    "\n",
    "#### With data from Kreckel et al. 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = MaximumLikelihood1D(pnlf,pn_bright['mOIII'],mhigh=27)\n",
    "mu = fitter([25])\n",
    "plot_pnlf(pn_kreckel['mOIII'],mu=29.91,completeness=27,binsize=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With data from Ciardullo+2002\n",
    "\n",
    "they measured \n",
    "* NGC3627: (m-M)=29.99+0.07-0.08 from 40 below completeness limit\n",
    "* NGC3351: (m-M)=30.05+0.08-0.16 from 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.use('TKAgg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.analyse import MaximumLikelihood1D, pnlf, cdf\n",
    "from pnlf.plot.pnlf import plot_pnlf\n",
    "from scipy.stats import kstest\n",
    "\n",
    "Mmax = -4.47\n",
    "\n",
    "data  = ascii.read(basedir/'data'/'external'/'Ciardullo_2002_NGC3627.txt')['OIII']\n",
    "# completeness according to the paper\n",
    "data = data[:40]\n",
    "completeness_limit = max(data)\n",
    "\n",
    "fitter = MaximumLikelihood1D(pnlf,data,mhigh=completeness_limit,Mmax=Mmax)\n",
    "mu,mu_p,mu_m = fitter([29])\n",
    "ks,pv = kstest(data,cdf,args=(mu,completeness_limit))\n",
    "\n",
    "print('{:.2f} + {:.2f} - {:.2f}'.format(mu,mu_p,mu_m))\n",
    "print(f'statistic={ks:.3f}, pvalue={pv:.3f}')\n",
    "\n",
    "binsize = (completeness_limit-Mmax-mu) / 4\n",
    "axes = plot_pnlf(data,mu,completeness_limit,binsize=binsize,\n",
    "                 mhigh=completeness_limit,Mmax=Mmax,color=tab10[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.analyse import MaximumLikelihood1D, pnlf, cdf\n",
    "from pnlf.plot.pnlf import plot_pnlf\n",
    "from scipy.stats import kstest\n",
    "\n",
    "Mmax = -4.47\n",
    "\n",
    "data  = ascii.read(basedir/'data'/'external'/'Ciardullo_2002_NGC3351.txt')['OIII']\n",
    "# completeness according to the paper\n",
    "data = data[:12]\n",
    "completeness_limit = max(data)\n",
    "\n",
    "fitter = MaximumLikelihood1D(pnlf,data,mhigh=completeness_limit,Mmax=Mmax)\n",
    "mu,mu_p,mu_m = fitter([29])\n",
    "ks,pv = kstest(data,cdf,args=(mu,completeness_limit))\n",
    "\n",
    "print('{:.2f} + {:.2f} - {:.2f}'.format(mu,mu_p,mu_m))\n",
    "print(f'statistic={ks:.3f}, pvalue={pv:.3f}')\n",
    "\n",
    "binsize = (completeness_limit-Mmax-mu) / 5\n",
    "axes = plot_pnlf(data,mu,completeness_limit,binsize=binsize,\n",
    "                 mhigh=completeness_limit,Mmax=Mmax,color=tab10[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Herrmann+2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.analyse import MaximumLikelihood1D, pnlf, cdf\n",
    "from pnlf.plot.pnlf import plot_pnlf\n",
    "from scipy.stats import kstest\n",
    "\n",
    "Mmax = -4.47\n",
    "\n",
    "data = ascii.read(basedir/'data'/'external'/'Herrmann_NGC5068_pn_candidates.txt')['m_5007']# completeness according to the paper\n",
    "data = data[:11]\n",
    "completeness_limit = max(data)\n",
    "\n",
    "fitter = MaximumLikelihood1D(pnlf,data,mhigh=completeness_limit,Mmax=Mmax)\n",
    "mu,mu_p,mu_m = fitter([29])\n",
    "ks,pv = kstest(data,cdf,args=(mu,completeness_limit))\n",
    "\n",
    "print('{:.2f} + {:.2f} - {:.2f}'.format(mu,mu_p,mu_m))\n",
    "print(f'statistic={ks:.3f}, pvalue={pv:.3f}')\n",
    "\n",
    "binsize = (completeness_limit-Mmax-mu) / 5\n",
    "axes = plot_pnlf(data,mu,completeness_limit,binsize=binsize,\n",
    "                 mhigh=completeness_limit,Mmax=Mmax,color=tab10[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### With least square  fitting\n",
    "\n",
    " - to use a least square approach we need to bin the data. \n",
    " + easier to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def pnlf(m,mu,N0):\n",
    "    '''planetary nebula luminosity function for curve_fit\n",
    "    \n",
    "    N(m) ~ e^0.307(m-mu) * (1-e^3(Mmax-m+mu))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m : ndarray\n",
    "        apparent magnitudes of the PNs\n",
    "        \n",
    "    mu : float\n",
    "        distance modulus\n",
    "        \n",
    "    N0 : float\n",
    "    '''\n",
    "    \n",
    "    m = np.atleast_1d(m)\n",
    "    \n",
    "    Mmax = -4.47\n",
    "    completneness = 28\n",
    "    normalization = -3.62866*np.exp(0.307*Mmax) + 3.25733*np.exp(0.307*completneness-0.307*mu) + 0.371333 * np.exp(3*Mmax - 2.693 * completneness + 2.693 * mu)\n",
    "    \n",
    "    out = N0*np.exp(0.307*(m-mu)) * (1-np.exp(3*(Mmax-m+mu))) / normalization\n",
    "    out[m>completneness] = 0\n",
    "    out[m<Mmax+mu] = 0\n",
    "    \n",
    "    return out\n",
    "\n",
    "def fit_pnlf(table):\n",
    "    \n",
    "    #table = table[table['type']=='PN']\n",
    "    \n",
    "    binsize = 0.2\n",
    "\n",
    "    guess = np.array([25,10])\n",
    "    \n",
    "    mlow = np.floor(np.min(table))\n",
    "    mhigh = np.ceil(np.max(table))\n",
    "    hist,bins  = np.histogram(table,np.arange(mlow,mhigh,binsize))\n",
    "    \n",
    "    \n",
    "    fit,sig = curve_fit(pnlf, bins[1:]+binsize/2,hist , guess)\n",
    "    mu, N0 = fit\n",
    "    print(f'mu={mu:.3f}, N0={N0:.2f}')\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(8,4))\n",
    "    \n",
    "    ax1.scatter(bins[:-1]+binsize/2,hist)\n",
    "    ax1.plot(bins[:-1]+binsize/2,pnlf(bins[:-1]+0.1,mu=mu,N0=N0),c='tab:orange',ls='--')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlim([25,28])\n",
    "    ax1.set_ylim([0.8,1.1*np.max(hist)])\n",
    "    ax1.set_xlabel('$m_{[\\mathrm{OIII}]}$')\n",
    "    ax1.set_ylabel('$N$')\n",
    "    \n",
    "    ax2.plot(bins[1:]+binsize/2,np.cumsum(hist))\n",
    "    ax2.plot(bins[1:]+binsize/2,np.cumsum(pnlf(bins[:-1]+binsize/2,mu=mu,N0=N0)),ls='--')\n",
    "    ax2.set_xlim([mlow,mhigh])\n",
    "    ax2.set_ylim([0,len(table)])\n",
    "    ax2.set_xlabel('$m_{[\\mathrm{OIII}]}$')\n",
    "    ax2.set_ylabel('Cumulative N')\n",
    "    \n",
    "fit_pnlf(tbl[tbl['type']=='PN']['mOIII'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance in parsec\n",
    "\n",
    "the measured distances are in the form of the distance modulus $\\mu = m-M$ which is the difference between apparent and absolute magnitude. By defintion of the absolte magnitude, we can convert this number into a distance in pc\n",
    "$$\n",
    "d = 10^{\\frac{\\mu}{5}+1} = 10 \\cdot \\exp\\left( \\ln 10 \\frac{\\mu}{5} \\right) \\\\\n",
    "\\delta d = \\frac{\\ln 10}{5} 10 \\exp\\left( \\ln 10 \\frac{\\mu}{5} \\right) \\delta \\mu = 0.2 \\ln 10 \\; d \\; \\delta \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_modulus_to_parsec(mu,mu_err=np.array([])):\n",
    "    \n",
    "    d = 10 * np.exp(np.log(10)*mu/5)\n",
    "    if len(mu_err) > 0:\n",
    "        d_err = 0.2 * np.log(10) * d * mu_err\n",
    "    print(f'd = ({d/1e6:.2f} + {d_err[0]/1e6:.2f} - {d_err[1]/1e6:.2f}) Mpc')\n",
    "    \n",
    "    return d, d_err\n",
    "\n",
    "d,d_err = distance_modulus_to_parsec(30.033,np.array([0.014,0.015]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "completeness = 29\n",
    "\n",
    "data = tbl[(tbl['type']=='PN') & (tbl['mOIII']<completeness)]['mOIII']\n",
    "err = tbl[(tbl['type']=='PN') & (tbl['mOIII']<completeness)]['dmOIII']\n",
    "\n",
    "fitter = MaximumLikelihood1D(pnlf,\n",
    "                             data,\n",
    "                             err=err,\n",
    "                             prior = prior,\n",
    "                             # additional parameters\n",
    "                             mhigh=completeness)\n",
    "\n",
    "# a good guess would be mu_guess = min(data)-Mmax\n",
    "mu = fitter([28])\n",
    "fitter.plot([29.5,30.25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no prior, no error:     29.966+0.031-0.095\n",
    "no prior, with error:   29.965+0.032-0.094\n",
    "with prior, no error:   29.950+0.038-0.071\n",
    "with prior, with error: 29.949+0.039-0.071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pnlf(tbl[criteria]['mOIII'],mu=mu,completeness=completeness,binsize=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 29.6\n",
    "xmax = 30.2\n",
    "x = np.linspace(xmin,xmax,1000)\n",
    "likelihood = np.exp([-fitter.likelihood(_) for _ in x])\n",
    "\n",
    "normalization = np.trapz(likelihood,x)\n",
    "\n",
    "integral = np.array([np.trapz(likelihood[x<=xp],x[x<=xp])/normalization for xp in x[1:]])\n",
    "\n",
    "plt.plot(x[1:],integral,label='cumulative likelihood')\n",
    "plt.axvline(mu,ls='--',c='k',lw=0.5)\n",
    "plt.axhline(0.5,ls='--',c='k',lw=0.5)\n",
    "\n",
    "plt.axhline(0.5+0.683/2,ls='--',c='k',lw=0.5)\n",
    "plt.axhline(0.5-0.683/2,ls='--',c='k',lw=0.5)\n",
    "\n",
    "\n",
    "mid = np.argmin(np.abs(integral-0.5))\n",
    "high = np.argmin(np.abs(integral-0.8415))\n",
    "low = np.argmin(np.abs(integral-0.1585))\n",
    "\n",
    "dp = x[high]-x[mid]\n",
    "dm = x[mid]-x[low]\n",
    "\n",
    "print(f'{x[mid]:.3f}+{dp:.3f}-{dm:.3f}')\n",
    "\n",
    "\n",
    "normal = gaussian(x,x[mid],dm)\n",
    "integral2 = np.array([np.trapz(normal[x<=xp],x[x<=xp]) for xp in x[1:]])\n",
    "plt.plot(x[1:],integral2,label=f'gaussian {dm:.2f}',ls='--')\n",
    "\n",
    "normal2 = gaussian(x,x[mid],dp)\n",
    "integral3 = np.array([np.trapz(normal2[x<=xp],x[x<=xp]) for xp in x[1:]])\n",
    "plt.plot(x[1:],integral3,label=f'gaussian {dp:.2f}',ls='--')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = MaximumLikelihood(pnlf,\n",
    "                           data,\n",
    "                           prior=None,\n",
    "                           mhigh=completeness)\n",
    "\n",
    "def prior(mu):\n",
    "    mu0 = 29.91\n",
    "    std = 0.8\n",
    "    \n",
    "    return 1 / (std*np.sqrt(2*np.pi)) * np.exp(-(mu-mu0)**2 / (2*std**2))\n",
    "\n",
    "sample_size = 0.5\n",
    "\n",
    "\n",
    "print(f'initial sample size {len(fitter.data)}')\n",
    "mu = np.linspace(29.5,30.5,500)\n",
    "full = np.exp(np.array([-fitter._loglike([_],fitter.data) for _ in mu]))\n",
    "\n",
    "data_sample = np.random.choice(fitter.data,int(sample_size*len(fitter.data)))\n",
    "sample = np.exp(np.array([-fitter._loglike([_],data_sample) for _ in mu]))\n",
    "\n",
    "valid = ~np.isnan(full) & ~np.isnan(sample)\n",
    "full /= np.abs(np.trapz(full[valid],mu[valid]))\n",
    "sample /= np.abs(np.trapz(sample[valid],mu[valid]))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot()\n",
    "ax.plot(mu[valid],full[valid],label='full')\n",
    "ax.plot(mu[valid],sample[valid],label='sample')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set_label('log likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_confidence_interval(quantile):\n",
    "    mid = np.argmin(np.abs(integral-0.5))\n",
    "    high = np.argmin(np.abs(integral-(0.5+quantile/2)))\n",
    "    low = np.argmin(np.abs(integral-(0.5-quantile/2)))\n",
    "\n",
    "    dp = integral[high]-integral[mid]\n",
    "    dm = integral[mid]-integral[low]\n",
    "\n",
    "    print(f'+{dp:.3f}-{dm:.3f}')\n",
    "    \n",
    "symmetric_confidence_interval(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 5\n",
    "size = 1000\n",
    "param = 29.9\n",
    "kwargs = {'mhigh':27}\n",
    "\n",
    "\n",
    "idx_low = np.argmin(data)\n",
    "idx_high = np.argmax(data)\n",
    "grid = np.linspace(data[idx_low]-width*err[idx_low],data[idx_high]+width*err[idx_high],size)\n",
    "pnlf_grid = pnlf(grid,param,**kwargs)\n",
    "\n",
    "-np.sum(np.log([np.trapz(pnlf(grid,param,**kwargs)*gaussian(grid,d,e),grid) for d,e in zip(data,err)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "-np.sum(np.log([np.trapz(pnlf(grid,param,**kwargs)*gaussian(grid,d,e),grid) for d,e in zip(data,err)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "-np.sum(np.log([quad(lambda x: pnlf(x,param,**kwargs)*gaussian(x,d,e),d-width*e,d+width*e)[0] for d,e in zip(data,err)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "print(len(fitter.data))\n",
    "fitter.evidence(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input\n",
    "\n",
    "`fit_function`\n",
    "\n",
    "`prior`\n",
    "\n",
    "`data`\n",
    "\n",
    "`error`\n",
    "\n",
    "what to do\n",
    "\n",
    "integrate `fit_function` * `gaussian`(data,error)\n",
    "\n",
    "calculate -sum log integrated function\n",
    "\n",
    "minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot import create_RGB\n",
    "\n",
    "# ====== define input parameters =============================\n",
    "rgb = create_RGB(NGC628.SII6716,NGC628.HA6562,NGC628.OIII5006)\n",
    "labels=['SII6716','HA6562','OIII5006']\n",
    "wcs=NGC628.wcs\n",
    "# ============================================================\n",
    "\n",
    "# create an empty figure with correct projection\n",
    "fig, ax = plt.subplots(figsize=(20,20),subplot_kw={'projection':wcs})\n",
    "\n",
    "# plot the image\n",
    "plt.imshow(rgb,origin='lower')\n",
    "\n",
    "# create a legend\n",
    "if labels:\n",
    "    # first we create a legend with three invisible handles\n",
    "    handles = 3*[mpl.patches.Rectangle((0, 0), 0, 0, alpha=0.0)]\n",
    "    leg = ax.legend(handles,labels, frameon=False,handlelength=0,prop={'size': 16})\n",
    "\n",
    "    # next we set the color of the three labels\n",
    "    for color,text in zip(['red','green','blue'],leg.get_texts()):\n",
    "        text.set_color(color)\n",
    "\n",
    "plt.savefig(basedir / 'reports' / f'{NGC628.name}_rgb.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copt\n",
    "\n",
    "use the convolved-optimised (copt) maps instead. This makes things a lot easier as we do not need to keep track of the different pointings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NGC1433'\n",
    "\n",
    "data_ext = Path('a:')\n",
    "\n",
    "class ReadLineMaps:\n",
    "    \n",
    "    def __init__(self,filename,extensions=['HB4861','OIII5006','HA6562','NII6583','SII6716','SII6730']):\n",
    "        \n",
    "        name, fwhm = filename.stem.split('-')\n",
    "        fwhm = float(fwhm[:4])*5\n",
    "\n",
    "        logger.info(f'loading {name}')\n",
    "\n",
    "        setattr(self,'name',name)        \n",
    "        setattr(self,'fwhm',fwhm)\n",
    "\n",
    "        with fits.open(filename) as hdul:\n",
    "\n",
    "            # save the white-light image\n",
    "            header = hdul[f'FLUX'].header\n",
    "            setattr(self,'header',header)\n",
    "            setattr(self,'wcs',WCS(header))\n",
    "            setattr(self,'shape',(header['NAXIS2'],header['NAXIS1']))\n",
    "            setattr(self,'Ebv_stars',hdul['EBV_STARS'].data)\n",
    "            setattr(self,'whitelight',hdul['FLUX'].data)\n",
    "            setattr(self,'whitelight_err',hdul['SNR'].data)\n",
    "\n",
    "            self.lines = []\n",
    "            \n",
    "            for line in extensions:\n",
    "                setattr(self,line,hdul[f'{line}_FLUX'].data)\n",
    "                setattr(self,f'{line}_err',hdul[f'{line}_FLUX_ERR'].data)                \n",
    "                setattr(self,f'{line}_SIGMA',np.sqrt(hdul[f'{line}_SIGMA'].data**2 - hdul[f'{line}_SIGMA_CORR'].data**2))\n",
    "                setattr(self,f'{line}_SIGMA_ERR',hdul[f'{line}_SIGMA_ERR'])\n",
    "\n",
    "                # append to list of available lines\n",
    "                self.lines.append(line)\n",
    "                \n",
    "filename = [x for x in (data_ext/'MUSE_DR2'/'copt').iterdir() if x.stem.startswith(name)][0]\n",
    "galaxy = ReadLineMaps(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import DAOStarFinder            # DAOFIND routine to detect sources\n",
    "from astropy.stats import sigma_clipped_stats  # calcualte statistics of images\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "mean, median, std = sigma_clipped_stats(galaxy.OIII5006, sigma=3.0,maxiters=5)\n",
    "correct_PSF = lambda lam: 5*3e-5*(lam-6483.58)\n",
    "\n",
    "# initialize and run StarFinder (DAOPHOT or IRAF)\n",
    "finder = DAOStarFinder(fwhm = (galaxy.fwhm - correct_PSF(5007)), \n",
    "                       threshold = threshold*std)\n",
    "peaks = finder(galaxy.OIII5006-median)\n",
    "print(f'{len(peaks)} objects in initial catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from photutils import CircularAperture         # define circular aperture\n",
    "from photutils import CircularAnnulus          # define annulus\n",
    "from photutils import aperture_photometry      # measure flux in aperture\n",
    "\n",
    "aperture_size = 2\n",
    "\n",
    "def light_in_gaussian(x,fwhm):\n",
    "\n",
    "    return 1-np.exp(-4*np.log(2)*x**2 / fwhm**2)\n",
    "\n",
    "if 'flux' in locals():\n",
    "    del flux\n",
    "\n",
    "out = {}\n",
    "for line in galaxy.lines:\n",
    "    print(line)\n",
    "    \n",
    "    wavelength = int(re.findall(r'\\d{4}', line)[0])\n",
    "    fwhm = galaxy.fwhm - correct_PSF(wavelength)\n",
    "    r = aperture_size*fwhm/2\n",
    "            \n",
    "    data  = getattr(galaxy,f'{line}').copy()\n",
    "    error = getattr(galaxy,f'{line}_err').copy()\n",
    "\n",
    "    positions = np.transpose((peaks['xcentroid'], peaks['ycentroid']))\n",
    "    aperture = CircularAperture(positions, r=r)\n",
    "    \n",
    "    phot = aperture_photometry(data, aperture, error = error)\n",
    "\n",
    "    r_in  = 4 * fwhm / 2 \n",
    "    r_out = np.sqrt(5*r**2+r_in**2)\n",
    "    annulus_aperture = CircularAnnulus(positions, r_in=r_in, r_out=r_out)\n",
    "    annulus_masks = annulus_aperture.to_mask(method='center')\n",
    "\n",
    "    bkg_median = []\n",
    "    bkg_median_no_clip = []\n",
    "    for mask in annulus_masks:\n",
    "        annulus_data = mask.multiply(data)\n",
    "        annulus_data_1d = annulus_data[mask.data > 0]\n",
    "        median_sigclip,_ , _ = sigma_clipped_stats(annulus_data_1d[~np.isnan(annulus_data_1d)],sigma=3,maxiters=3)          \n",
    "        bkg_median_no_clip.append(np.nanmedian(annulus_data_1d[~np.isnan(annulus_data_1d)]))          \n",
    "        bkg_median.append(median_sigclip)\n",
    "    phot['bkg_median'] = np.array(bkg_median) \n",
    "    phot['bkg_local'] = phot['bkg_median'] * aperture.area\n",
    "    phot['flux'] = phot['aperture_sum'] - phot['bkg_local']   \n",
    "    phot['flux'] /= light_in_gaussian(r,fwhm)\n",
    "    \n",
    "    out[line] = phot\n",
    "\n",
    "for k,v in out.items():\n",
    "\n",
    "    # first we create the output table with \n",
    "    if 'flux' not in locals():\n",
    "        flux = v[['id','xcenter','ycenter']]\n",
    "        flux.rename_column('xcenter','x')\n",
    "        flux.rename_column('ycenter','y')\n",
    "        flux['x'] = flux['x'].value         # we don't want them to be in pixel units\n",
    "        flux['y'] = flux['y'].value\n",
    "    flux[k] = v['flux'] \n",
    "    flux[f'{k}_err'] = v['aperture_sum_err']\n",
    "flux['mOIII'] = -2.5*np.log10(flux['OIII5006']*1e-20) - 13.74\n",
    "flux['dmOIII'] = np.abs( 2.5/np.log(10) * flux['OIII5006_err'] / flux['OIII5006'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.analyse import emission_line_diagnostics\n",
    "\n",
    "galaxy.mu = parameters[name]['mu']\n",
    "completeness_limit = parameters[name]['completeness_limit']\n",
    "\n",
    "flux['HA6562_SIGMA'] = 0\n",
    "print(f'emission line diagnostics for {galaxy.name}')\n",
    "tbl = emission_line_diagnostics(flux,galaxy.mu,completeness_limit) \n",
    "\n",
    "# create additional columns that are needed for the classification\n",
    "tbl['sharp'] = peaks['sharpness']\n",
    "tbl['round'] = peaks['roundness2']\n",
    "tbl['SkyCoord'] = SkyCoord.from_pixel(tbl['x'],tbl['y'],galaxy.wcs)\n",
    "tbl['exclude'] = False\n",
    "tbl['overluminous'] = False\n",
    "\n",
    "slow  = .2 #galaxy.sharplo  \n",
    "shigh = 1. #galaxy.sharphi \n",
    "r     = .8 #galaxy.roundness \n",
    "\n",
    "# table contains all detected objects. here we mask all undesired objects.\n",
    "c_shape = ((tbl['sharp']>slow) & (tbl['sharp']<shigh) & (np.abs(tbl['round'])<r)) \n",
    "c_PN    = (tbl['type']=='PN')\n",
    "c_SNR   = (tbl['SNRorPN'] & (tbl['type']=='SNR'))\n",
    "c_detec = tbl['OIII5006_detection'] \n",
    "c_limit = (tbl['mOIII']<completeness_limit) \n",
    "\n",
    "dPSF = 0.153\n",
    "\n",
    "# we underestimate the errors. This value comes from a theoreticla SII ratio (see function)\n",
    "tbl['dmOIII'] *= 1.67\n",
    "tbl['dmOIII'] = np.sqrt(tbl['dmOIII']**2 + dPSF**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.analyse import MaximumLikelihood1D, pnlf, cdf\n",
    "from pnlf.plot.pnlf import plot_pnlf\n",
    "from pnlf.auxiliary import mu_to_parsec\n",
    "from scipy.stats import kstest\n",
    "\n",
    "binsize = 0.4\n",
    "Mmax = -4.47\n",
    "\n",
    "criteria = c_shape & (c_PN) & c_detec & ~tbl['exclude'] & ~tbl['overluminous']\n",
    "data = tbl[np.where(criteria & c_limit)]['mOIII']\n",
    "err = tbl[np.where(criteria & c_limit)]['dmOIII']\n",
    "\n",
    "print(f'completeness limit = {completeness_limit}, binsize = {binsize}')\n",
    "fitter = MaximumLikelihood1D(pnlf,data,err=err,mhigh=completeness_limit,Mmax=Mmax)\n",
    "galaxy.mu,mu_p,mu_m = fitter([29])\n",
    "print('{:.2f} + {:.2f} - {:.2f}'.format(galaxy.mu,mu_p,mu_m))\n",
    "\n",
    "ks,pv = kstest(data,cdf,args=(galaxy.mu,completeness_limit))\n",
    "print(f'{name}: statistic={ks:.3f}, pvalue={pv:.3f}')\n",
    "\n",
    "#Plot PNLF\n",
    "axes = plot_pnlf(tbl[criteria]['mOIII'],galaxy.mu,completeness_limit,\n",
    "                 binsize=binsize,mhigh=28.5,Mmax=Mmax,filename=Path('test'),color=tab10[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = basedir / 'data' / 'catalogues' / 'NGC0628_PN_candidates.txt'\n",
    "tbl = ascii.read(filename, format='fixed_width_two_line') \n",
    "\n",
    "data = list(tbl['mOIII'])\n",
    "err  = list(tbl['dmOIII'])\n",
    "\n",
    "#data.append(25.2)\n",
    "#err.append(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from scipy.optimize import minimize\n",
    "from inspect import signature\n",
    "import corner\n",
    "\n",
    "from pnlf.analyse import MaximumLikelihood1D, pnlf\n",
    "\n",
    "\n",
    "class MaximumLikelihood:\n",
    "    '''\n",
    "\n",
    "    for uncertainties \n",
    "    https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : function\n",
    "        PDF of the form `func(data,params)`. `func` must accept a\n",
    "        ndarray for `data` and can have any number of additional\n",
    "        parameters (at least one).\n",
    "        \n",
    "    data : ndarray\n",
    "        Measured data that are feed into `func`.\n",
    "\n",
    "    err : ndarray\n",
    "        Error associated with data.\n",
    "\n",
    "    prior : function\n",
    "        Prior probabilities for the parameters of func.\n",
    "\n",
    "    method : \n",
    "        algorithm that is used for the minimization.\n",
    "\n",
    "    **kwargs\n",
    "       additional fixed key word arguments that are passed to func.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,func,data,err=None,prior=None,method='Nelder-Mead',**kwargs):\n",
    "        \n",
    "        if len(signature(func).parameters)-len(kwargs)<2:\n",
    "            raise ValueError(f'`func` must have at least one free argument')\n",
    "        \n",
    "        self.func   = func\n",
    "        self.labels = list(signature(func).parameters.keys())[1:]\n",
    "        self.data   = data\n",
    "        self.err    = err\n",
    "        self.prior  = prior\n",
    "        self.method = method\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        logger.info(f'searching best parameters for {self.labels} with {len(self.data)} data points')\n",
    "\n",
    "    def log_likelihood(self,params,data):\n",
    "        '''calculate the log liklihood of the given parameters\n",
    "        \n",
    "        This function takes the previously specified PDF and calculates\n",
    "        the sum of the logarithmic probabilities. If key word arguments\n",
    "        were initially passed to the class, they are also passed to the\n",
    "        function\n",
    "        '''\n",
    "        return np.sum(np.log(self.func(data,*params,**self.kwargs)))\n",
    "    \n",
    "    def log_prior(self,params):\n",
    "        '''log of priors'''\n",
    "        \n",
    "        lp = self.prior(*params)\n",
    "        if lp == 0:\n",
    "            return -np.inf\n",
    "        else:\n",
    "            return np.log(lp)\n",
    "\n",
    "    def log_probability(self,params,data):\n",
    "        if self.prior:\n",
    "            return self.log_likelihood(params,data)+self.log_prior(params)\n",
    "        else:\n",
    "            return self.log_likelihood(params,data)\n",
    "        \n",
    "    def neg_log_probability(self,params,data):\n",
    "        if self.prior:\n",
    "            return -self.log_likelihood(params,data)+self.log_prior(params)\n",
    "        else:\n",
    "            return -self.log_likelihood(params,data)\n",
    "        \n",
    "    def fit(self,guess,nwalkers=10,nsteps=1000):\n",
    "        '''use scipy minimize to find the best parameters'''\n",
    "        \n",
    "        self.soln = minimize(self.neg_log_probability,guess,args=(self.data,),method=self.method)\n",
    "        if not self.soln.success:\n",
    "            raise RuntimeError('fit was not successful')\n",
    "        self.x = self.soln.x\n",
    "\n",
    "        ndim = len(guess)\n",
    "        pos = np.random.normal(self.soln.x,0.1,size=(nwalkers,ndim))\n",
    "\n",
    "        self.sampler = emcee.EnsembleSampler(nwalkers, ndim,self.log_probability,args=(self.data,))\n",
    "        state = self.sampler.run_mcmc(pos, 100)\n",
    "        self.sampler.reset()\n",
    "        self.sampler.run_mcmc(state,nsteps,progress=True)\n",
    "\n",
    "        flat_samples = self.sampler.get_chain(discard=100,thin=5, flat=True)\n",
    "\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "            mcmc[1] = self.soln.x[i]\n",
    "            q = np.diff(mcmc)\n",
    "            #truth.append(mcmc[1])\n",
    "            print(f'{self.labels[i]}= {mcmc[1]:.3f}+{q[1]:.3f}-{q[0]:.3f}') \n",
    "        \n",
    "        fig = corner.corner(flat_samples,bins=100, labels=self.labels,truths=self.x)\n",
    "        \n",
    "        return self.x\n",
    "\n",
    "    def __call__(self,guess,**kwargs):\n",
    "        '''use scipy minimize to find the best parameters'''\n",
    "\n",
    "        return self.fit(guess,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE = MaximumLikelihood(pnlf,\n",
    "                        data,\n",
    "                        mhigh=28\n",
    "                        )\n",
    "                        #err=err,\n",
    "                        #mhigh=galaxy.completeness_limit)\n",
    "\n",
    "MLE.fit([29],nwalkers=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append(25.2)\n",
    "err.append(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "from scipy.optimize import minimize\n",
    "from inspect import signature\n",
    "import corner\n",
    "\n",
    "from pnlf.analyse import MaximumLikelihood1D, pnlf\n",
    "\n",
    "\n",
    "\n",
    "mu = 29\n",
    "Pbad = 0.01         # fraction of bad points\n",
    "Yb   = 25           # mean of the bad points\n",
    "Vb   = 1            # variance of the bad points\n",
    "qi = len(data)*[1]  # 1 if point is good, 0 else\n",
    "\n",
    "guess = np.array([mu] + [Pbad] + [Yb] + [Vb] + qi) \n",
    "\n",
    "nwalkers=500\n",
    "nsteps=5000\n",
    "\n",
    "\n",
    "def log_prior(params):\n",
    "    '''log of priors'''\n",
    "\n",
    "    mu0 = 29.91\n",
    "    std = 0.3\n",
    "    \n",
    "    mu, Pbad, Yb, Vb, *qi = params\n",
    "    qi = np.array(qi)\n",
    "    qi[qi>=1] = 1\n",
    "    qi[qi<1]  = 0\n",
    "        \n",
    "    if Pbad <0: p_bad = 0\n",
    "    else: p_bad = 2 / (0.02*np.sqrt(2*np.pi)) * np.exp(-(Pbad)**2 / (2*0.02**2))\n",
    "\n",
    "    p_mu = 1 / (std*np.sqrt(2*np.pi)) * np.exp(-(mu-mu0)**2 / (2*std**2))\n",
    "    p_qi = np.prod((1-Pbad)**qi*Pbad**(1-qi))\n",
    "    \n",
    "    if p_mu*p_qi*p_bad <= 0:\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return np.log(p_mu*p_qi*p_bad)\n",
    "\n",
    "def log_likelihood(params,data):\n",
    "    mu, Pbad, Yb, Vb, *qi = params\n",
    "    qi = np.array(qi)\n",
    "    qi[qi>=1] = 1\n",
    "    qi[qi<1]  = 0\n",
    "        \n",
    "    log_pnlf = qi*np.log(pnlf(data,mu,mhigh=galaxy.completeness_limit))\n",
    "    log_bad  = (1-qi)*np.log(1/np.sqrt(2*np.pi*Vb) * np.exp(-(data-Yb)**2/(2*Vb)))\n",
    "    \n",
    "    return np.sum(log_pnlf+log_bad)\n",
    "    \n",
    "def log_prob_fn(params,data):\n",
    "    p = log_likelihood(params,data)+log_prior(params)\n",
    "    if np.isnan(p):\n",
    "        return -np.inf\n",
    "    return p\n",
    "\n",
    "\n",
    "#soln = minimize(neg_log_probability,guess,args=(data,),method='Nelder-Mead')\n",
    "#if not soln.success:\n",
    "#    raise RuntimeError('fit was not successful')\n",
    "#x = soln.x\n",
    "\n",
    "ndim = len(guess)\n",
    "pos = np.random.normal(guess,[0.1,0.01,0.5,0.5]+len(data)*[0.001],size=(nwalkers,ndim))\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim,log_prob_fn,args=(data,))\n",
    "state = sampler.run_mcmc(pos, 100)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(state,nsteps,progress=True)\n",
    "\n",
    "flat_samples = sampler.get_chain(discard=100,thin=5, flat=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "labels = ['mu','Pbad','Yb','Vb']\n",
    "for i in range(len(labels)):\n",
    "    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{{2:.3f}}}\"\n",
    "    txt = txt.format(mcmc[1], q[0], q[1], labels[i])\n",
    "    display(Math(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qi_p = np.percentile(flat_samples,50,axis=0)\n",
    "np.sum(qi_p>1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture model\n",
    "\n",
    "https://dfm.io/posts/mixture-models/\n",
    "https://emcee.readthedocs.io/en/stable/user/blobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "mhigh = 28\n",
    "\n",
    "# Define the probabilistic model...\n",
    "# A simple prior:\n",
    "bounds = [(27, 32), (0, 0.2), (20, 30), (0.1, 5)]\n",
    "def lnprior(p):\n",
    "    # We'll just put reasonable uniform priors on all the parameters.\n",
    "    if not all(b[0] < v < b[1] for v, b in zip(p, bounds)):\n",
    "        return -np.inf\n",
    "    return 0\n",
    "\n",
    "# The \"foreground\" linear likelihood:\n",
    "def lnlike_fg(p,data):\n",
    "    mu, _, _, _ = p\n",
    "    \n",
    "    return np.log(pnlf(data,mu,mhigh=mhigh))\n",
    "\n",
    "# The \"background\" outlier likelihood:\n",
    "def lnlike_bg(p,data):\n",
    "    _, Q, M, lnV = p\n",
    "    \n",
    "    return -0.5 * ((M - data) ** 2 / np.exp(lnV) + lnV)\n",
    "\n",
    "# Full probabilistic model.\n",
    "def lnprob(p,data):\n",
    "    mu, Q, M, lnV = p\n",
    "    \n",
    "    # First check the prior.\n",
    "    lp = lnprior(p)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf, (None,None)\n",
    "    \n",
    "    # Compute the vector of foreground likelihoods and include the q prior.\n",
    "    ll_fg = lnlike_fg(p,data)\n",
    "    arg1 = ll_fg + np.log(Q)\n",
    "    \n",
    "    # Compute the vector of background likelihoods and include the q prior.\n",
    "    ll_bg = lnlike_bg(p,data)\n",
    "    arg2 = ll_bg + np.log(1.0 - Q)\n",
    "    \n",
    "    # Combine these using log-add-exp for numerical stability.\n",
    "    ll = np.sum(np.logaddexp(arg1, arg2))\n",
    "    \n",
    "    # We're using emcee's \"blobs\" feature in order to keep track of the\n",
    "    # foreground and background likelihoods for reasons that will become\n",
    "    # clear soon.\n",
    "    return lp + ll, (arg1, arg2)\n",
    "\n",
    "# Initialize the walkers at a reasonable location.\n",
    "ndim, nwalkers = 4, 32\n",
    "p0 = np.random.normal([29,0.05,25,1],[0.2,0.01,0.5,0.5],size=(nwalkers,ndim))\n",
    "\n",
    "\n",
    "# Set up the sampler.\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob,args=(data,),blobs_dtype=[('args',tuple)])\n",
    "\n",
    "# Run a burn-in chain and save the final location.\n",
    "pos, _, _, _ = sampler.run_mcmc(p0, 500)\n",
    "\n",
    "# Run the production chain.\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(pos, 1500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "flat_samples = sampler.get_chain(discard=100,thin=5, flat=True)\n",
    "\n",
    "labels = ['mu','Pbad','Yb','Vb']\n",
    "for i in range(len(labels)):\n",
    "    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{{2:.3f}}}\"\n",
    "    txt = txt.format(mcmc[1], q[0], q[1], labels[i])\n",
    "    display(Math(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 0.0\n",
    "post_prob = np.zeros(len(data))\n",
    "for i in range(sampler.chain.shape[1]):\n",
    "    for j in range(sampler.chain.shape[0]):\n",
    "        try:\n",
    "            ll_fg, ll_bg = sampler.blobs[i][j]\n",
    "            post_prob += np.exp(ll_fg - np.logaddexp(ll_fg, ll_bg))\n",
    "            norm += 1\n",
    "        except:\n",
    "            pass\n",
    "post_prob /= norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(map(\"{0:.3f}\".format, post_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare DR1 and DR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.io import read_catalogue\n",
    "from astropy.coordinates import match_coordinates_sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = basedir / 'data' / 'DR2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR1 = ascii.read(folder/'NGC0628_nebulae_DR1.txt',format='fixed_width',delimiter='\\t')\n",
    "DR2 = ascii.read(folder/'NGC0628_nebulae_DR2.txt',format='fixed_width',delimiter='\\t')\n",
    "\n",
    "DR1['SkyCoord'] = SkyCoord(DR1['RaDec'])\n",
    "DR2['SkyCoord'] = SkyCoord(DR2['RaDec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = '0.5\"'\n",
    "\n",
    "fig = plt.figure(figsize=(9,3))\n",
    "\n",
    "\n",
    "for i,t in enumerate(['PN','SNR','HII']):\n",
    "    \n",
    "    ax = fig.add_subplot(1,3,i+1)\n",
    "    sub = DR1[DR1['type']==t].copy()\n",
    "    \n",
    "    ID, angle, Quantity  = match_coordinates_sky(sub['SkyCoord'],DR2['SkyCoord'])\n",
    "    within_tolerance = len(angle[angle.__lt__(Angle(tolerance))])\n",
    "    mask = angle.__lt__(Angle(tolerance))\n",
    "    \n",
    "    sub['mOIIIDR2'] = DR2[ID]['mOIII']\n",
    "    sub['typeDR2']  = DR2[ID]['type']\n",
    "    \n",
    "    #sub = sub[mask]\n",
    "    \n",
    "    for col,ty in zip(['tab:red','tab:blue','tab:orange'],['PN','SNR','HII']):\n",
    "        subsub = sub[(sub['typeDR2']==ty) & mask]\n",
    "        ax.scatter(subsub['mOIII'],subsub['mOIIIDR2'],label=f'{ty} in DR2')\n",
    "    \n",
    "    ax.scatter(sub[~mask]['mOIII'],sub[~mask]['mOIII'],\n",
    "               s=15, facecolors='none', edgecolors='gray',label='missing')\n",
    "        \n",
    "    xmin,xmax=ax.get_xlim()\n",
    "    ymin,ymax=ax.get_ylim()\n",
    "    \n",
    "    ax.set_title(f'{t} in DR1')\n",
    "    ax.plot([min(xmin,ymin),max(xmax,ymax)],[min(xmin,ymin),max(xmax,ymax)],'k--')\n",
    "\n",
    "    ax.set_xlabel('mOIII DR1')\n",
    "    ax.set_ylabel('mOIII DR2')\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    print(f'{t}: {within_tolerance} of {len(angle)} match within {tolerance}\": {within_tolerance / len(angle)*100:.1f} %')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star Mask from GAIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=ascii.read(basedir/'reports'/'sample.txt')\n",
    "sample['SkyCoord'] = SkyCoord(sample['R.A.'],sample['Dec.'])\n",
    "sample.add_index('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.gaia import Gaia\n",
    "\n",
    "\n",
    "radius = u.Quantity(4.0, u.arcmin)\n",
    "j = Gaia.cone_search_async(sample.loc['NGC0628']['SkyCoord'], radius)\n",
    "stars = j.get_results()\n",
    "print(f'{len(stars)} stars found within {radius}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars['SkyCoord'] = SkyCoord(stars['ra'],stars['dec'])\n",
    "x,y = stars['SkyCoord'].to_pixel(galaxy.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(two_column,two_column))\n",
    "ax  = fig.add_subplot(111,projection=galaxy.wcs)\n",
    "\n",
    "norm = simple_norm(galaxy.whitelight,clip=False,percent=99)\n",
    "ax.imshow(galaxy.whitelight,norm=norm)\n",
    "\n",
    "for xp,yp in zip(x,y):\n",
    "    if not np.isnan(galaxy.PSF[int(xp),int(yp)]):\n",
    "        ax.scatter(xp,yp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional stuff to parameters.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir / 'data' / 'interim' / 'parameters.yml', 'w') as outfile:\n",
    "    yaml.dump(parameters, outfile, default_flow_style=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
